{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f437719c-4926-425f-9516-42a94a705f0e",
   "metadata": {},
   "source": [
    "# SPMpy \n",
    "* Authors : Dr. Jewook Park at CNMS, ORNL\n",
    "    * Center for Nanophase Materials Sciences (CNMS), Oak Ridge National Laboratory (ORNL)\n",
    "    * email :  parkj1@ornl.gov\n",
    "        \n",
    "> **SPMpy** is a python package to analysis scanning probe microscopy (SPM) data analysis, such as scanning tunneling microscopy and spectroscopy (STM/S) data and atomic force microscopy (AFM) images, which are inherently multidimensional. SPMpy exploits recent image processing(a.k.a. Computer Vision) techniques, and utilzes [building blocks](https://scipy-lectures.org/intro/intro.html#the-scientific-python-ecosystem) and excellent visualization tools in the [scientific python ecosystem](https://holoviz.org/index.html). Many parts are inspired by well-known SPM data analysis programs, for example, [Wsxm](http://www.wsxm.eu/) and [Gwyddion](http://gwyddion.net/). SPMpy is trying to apply lessons from [Fundamentals in Data Visualization](https://clauswilke.com/dataviz/).\n",
    "\n",
    ">  **SPMpy** is an open-source project. (Github: https://github.com/Jewook-Park/SPMPY )\n",
    "> * Contributions, comments, ideas, and error reports are always welcome. Please use the Github page or email parkj1@ornl.gov. Comments & remarks should be in Korean or English. \n",
    "# SPMpy data analysis function \n",
    "\n",
    "* To use SPMpy functions, SPM data() need to be converted as PANDAS DataFrame or Xarray DataSet. \n",
    "\n",
    "> * check **SPMpy_fileloading_functions** first."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84db7421-017c-48d1-bd78-efd28a99e26f",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 0.  Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d1aa3d-756e-45e3-a9dd-812c00eb04b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "from warnings import warn\n",
    "from scipy import signal\n",
    "\n",
    "import math\n",
    "import skimage\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "#from SPMpy_2D_data_analysis_funcs import files_in_folder,img2xr,grid2xr,gridline2xr,gwy_img2df,gwy_df_ch2xr\n",
    "\n",
    "\n",
    "# some packages may be yet to be installed\n",
    "try:\n",
    "     from pptx import Presentation\n",
    "except ModuleNotFoundError:\n",
    "    warn('ModuleNotFoundError: No module named Presentation')\n",
    "    !pip install python-pptx  \n",
    "    from pptx import Presentation\n",
    "    from pptx.util import Inches, Pt\n",
    "\n",
    "try:\n",
    "    import nanonispy as nap\n",
    "except ModuleNotFoundError:\n",
    "    warn('ModuleNotFoundError: No module named nanonispy')\n",
    "    !pip install nanonispy\n",
    "    import nanonispy as nap\n",
    "\n",
    "try:\n",
    "    import seaborn_image as isns\n",
    "except ModuleNotFoundError:\n",
    "    warn('ModuleNotFoundError: No module named seaborn-image')\n",
    "    #!pip install --upgrade scikit-image == 0.19.0.dev0\n",
    "    !pip install --upgrade seaborn-image    \n",
    "    import seaborn_image as isns\n",
    "\n",
    "try:\n",
    "    import xarray as xr\n",
    "except ModuleNotFoundError:\n",
    "    warn('ModuleNotFoundError: No module named xarray')\n",
    "    #!pip install --upgrade scikit-image == 0.19.0.dev0\n",
    "    !pip install xarray \n",
    "    import xarray as xr\n",
    "    \n",
    "try:\n",
    "    import xrft\n",
    "except ModuleNotFoundError:\n",
    "    warn('ModuleNotFoundError: No module named xrft')\n",
    "    !pip install xrft \n",
    "    import xrft\n",
    "\n",
    "try:\n",
    "    import seaborn_image as isns\n",
    "except ModuleNotFoundError:\n",
    "    warn('ModuleNotFoundError: No module named seaborn-image')\n",
    "    #!pip install --upgrade scikit-image == 0.19.0.dev0\n",
    "    !pip install --upgrade seaborn-image    \n",
    "    import seaborn_image as isns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcdc899-62f3-4d28-ae59-d713498377d1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Grid data analysis functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4dd53df-9874-486f-8dd6-bb35c76dc934",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grid_3D_unit_calc (grid_3D): \n",
    "    '''\n",
    "    Grid_3D data contains 'I_fb' & 'LIX_fb' \n",
    "    using numerical derivative of xr.differentiate \n",
    "    compare the ration between  real [A/V] unit and measured Lockin 'pA' unit.\n",
    "    convert and make a new channel 'LIX_unit_calc'\n",
    "    \n",
    "    '''\n",
    "    grid_3D_dIdV_numeric = grid_3D.differentiate(coord = 'bias_mV').I_fb\n",
    "    # numerically calculated dI/dV from I_fb\n",
    "    LIX_convert_ratio = grid_3D_dIdV_numeric / grid_3D.LIX_fb\n",
    "\n",
    "    grid_3D['LIX_unit_calc'] = np.abs( LIX_convert_ratio.mean())*grid_3D.LIX_fb\n",
    "    \n",
    "    #grid_3D\n",
    "\n",
    "    return grid_3D\n",
    "    # in case of LIX offset exist.. \n",
    "    # if it is metallic sample.. no problem "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61df17e3-ec89-4e30-bbbc-57bfa52e99b6",
   "metadata": {},
   "source": [
    "# Need to make a bias_mV = 0 adjust function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9801f41b-f732-4a5e-a246-cffd96db852a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##\n",
    "# find neares I =0 bias_mV \n",
    "def Bias_mV_offset_avg_test(grid_3D):\n",
    "    I_fb_avg_df = grid_3D.I_fb.mean (dim = ['X','Y']).to_dataframe().abs()\n",
    "    if I_fb_avg_df.I_fb.idxmin() == 0:\n",
    "        print ('Bias_mV is set to I = 0')\n",
    "    else:\n",
    "        print ('need to adjust Bias_mV Zero')\n",
    "        grid_3D = grid_3D.assign_coords(bias_mV= (  grid_3D.bias_mV - I_fb_avg_df.I_fb.idxmin()  ))\n",
    "        print ('Bias_mV Zero shifted : '+ str( round(I_fb_avg_df.I_fb.idxmin(),2)  ))\n",
    "    return grid_3D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fc6e12-10e2-48bb-b659-12566b79ecf8",
   "metadata": {
    "tags": []
   },
   "source": [
    "# find the gap region \n",
    "* gap size \n",
    "    * based on measurement error (I limit ~ 1E-11pA) or (Lock-in resolution limnit ~ 1E-11 pA ) find the gapped region in spectroscopy \n",
    "* calibrated LDOS \n",
    "    * convert the LIX values (from Lockin) as dI/dV unit [A/V] \n",
    "    * check the LIX offest value based on calibrated dI/dV at I=0\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0785dcae-96a2-40b9-8bab-0debba5a5583",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grid_3D_Gap(grid_3D, I_0_pA = 1E-13 ,LIX_0_pA = 1E-14):\n",
    "    '''\n",
    "    # simply assign the gap by using I & LIX reference 0 point. \n",
    "    \n",
    "    '''\n",
    "    grid_3D_prcssd = grid_3D.copy(deep = True)\n",
    "    # I_fb values less than I_min_A => zero value \n",
    "    I_0_pA = I_0_pA\n",
    "\n",
    "    gap_mask_I  = np.abs(grid_3D.I_fb) < I_0_pA\n",
    "    gap_I =  grid_3D.I_fb.where(gap_mask_I)\n",
    "\n",
    "    CBM_I_mV = grid_3D.bias_mV.where(gap_mask_I).max('bias_mV') # CBM_I_mV\n",
    "    VBM_I_mV = grid_3D.bias_mV.where(gap_mask_I).min('bias_mV') # VBM_I_mV\n",
    "    # map of the CBM&VBM energy (bias_mV)   \n",
    "    gap_size_I =  (CBM_I_mV - VBM_I_mV ) \n",
    "    # from VBM to CBM energy gap size in mV ( index differnce * bias_mV step size) \n",
    "\n",
    "    grid_3D_prcssd['CBM_I_mV'] = CBM_I_mV\n",
    "    grid_3D_prcssd['VBM_I_mV'] = VBM_I_mV\n",
    "    grid_3D_prcssd['gap_size_I'] = gap_size_I\n",
    "    ###############################################\n",
    "\n",
    "    grid_3D['dIdV'] = grid_3D.differentiate(coord = 'bias_mV').I_fb\n",
    "    # numerically calculated dI/dV from I_fb\n",
    "    LIX_ratio = grid_3D.dIdV / grid_3D.LIX_fb\n",
    "       \n",
    "    grid_3D['LIX_unit_calc'] = np.abs( LIX_ratio.mean())*grid_3D.LIX_fb\n",
    "    # LIX unit calibration \n",
    "    # pA unit : lock-in result \n",
    "    # LIX_unit_calc : calibrated as [A/V] unit for dI/dV\n",
    "    \n",
    "    \n",
    "    # LIX_fb values less than LIX_min_A => zero value \n",
    "    # LIX_0_pA = LIX_0_pA\n",
    "    LIX_0_AV  =  LIX_0_pA * LIX_ratio.mean()\n",
    "    # calibrated LIX resolution limit\n",
    "    gap_mask_LIX  = np.abs(grid_3D.LIX_unit_calc) < LIX_0_AV\n",
    "    # gap_mask_LIX  = np.abs(grid_3D.LIX_fb) > LIX_0_pA\n",
    "    # because of the same coefficient ('LIX_ratio.mean()')\n",
    "    # range for CBM &VBM is not different between  LIX_unit_calc & LIX_fb\n",
    "    # 3D mask \n",
    "\n",
    "    LIX_unit_calc_offst = grid_3D.dIdV.where(gap_mask_I).mean()- grid_3D['LIX_unit_calc'].where(gap_mask_I).mean()\n",
    "    # possible LIX offset adjust (based on dI/dV calc value)\n",
    "    grid_3D_prcssd['LDOS_fb'] = grid_3D.LIX_unit_calc + LIX_unit_calc_offst\n",
    "    # assign dI/dV value at I=0  as a reference offset \n",
    "    # grid_3D['LDOS_fb'] is calibrated dIdV with correct unit ([A/V]) for LDOS \n",
    "    # LDOS_fb is proportional to the real LDOS\n",
    "    # here we dont consider the matrix element for\n",
    "    grid_3D_prcssd['CBM_LIX_mV'] = grid_3D.bias_mV.where(gap_mask_LIX).max('bias_mV').fillna(0)\n",
    "    \n",
    "    #CBM_I_mV = grid_3D.bias_mV.where(gap_mask_I).max('bias_mV') # CBM_I_mV\n",
    "\n",
    "    \n",
    "    grid_3D_prcssd['VBM_LIX_mV'] = grid_3D.bias_mV.where(gap_mask_LIX).min('bias_mV').fillna(0)\n",
    "    # CBM& VBM bias_mV value ,  fill nan as (0)\n",
    "    gap_size_LIX = grid_3D_prcssd.CBM_LIX_mV - grid_3D_prcssd.VBM_LIX_mV\n",
    "    grid_3D_prcssd['gap_size_LIX'] = gap_size_LIX\n",
    "    # cf) use the 'isin' to find zero & find the index with 'np.argwhere'\n",
    "\n",
    "    # (c.f) to fill the VBM, \n",
    "    #grid_3D_prcssd['VBM_LIX'] = grid_3D.LIX_fb.where(gap_mask_LIX).fillna(10000).argmin(dim = 'bias_mV')\n",
    "    # apply find gap_mask_LIX(gapped area), fill 'nan' as -10000 & find argmax \n",
    "    #grid_3D_prcssd['VBM_LIX'] = grid_3D.VBM_LIX.where(~gap_mask_LIX).fillna(np.argwhere(grid_3D_prcssd.bias_mV.isin(0).values)[0,0])\n",
    "    # for the metallic area (~gap_mask_LIX), fill the bias_mV index of 'zero_bias'\n",
    "    #grid_3D_prcssd['VBM_LIX']=  grid_3D_prcssd['VBM_LIX'].astype(np.int64)\n",
    "    # the same dtype as np.int64\n",
    "    grid_3D_prcssd.attrs['I[A]_limit'] = I_0_pA\n",
    "    grid_3D_prcssd.attrs['LDOS[A/V]_limit'] = LIX_0_AV.values\n",
    "    \n",
    "    ## split  LIX_fb_offst as a CB & VB region \n",
    "    \n",
    "    grid_3D_prcssd['LDOS_fb_CB'] =  grid_3D_prcssd.LDOS_fb.where(\n",
    "        grid_3D_prcssd.bias_mV  >  grid_3D_prcssd.CBM_LIX_mV\n",
    "    )\n",
    "    grid_3D_prcssd['LDOS_fb_VB'] =  grid_3D_prcssd.LDOS_fb.where(\n",
    "        grid_3D_prcssd.bias_mV  <  grid_3D_prcssd.VBM_LIX_mV\n",
    "    )\n",
    "    \n",
    "    return grid_3D_prcssd\n",
    "\n",
    "#test\n",
    "#grid_3D_gap = grid_3D_Gap(grid_3D)\n",
    "#grid_3D_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e5262367-a210-40c4-bf6c-6f3794abca94",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grid_3D_SCgap(xr_data,tolerance_I =  0.2E-11, tolerance_LIX = 1E-11,\n",
    "                  apply_SGfilter = True,  window_length = 21, polyorder = 3, \n",
    "                  bias_mV_set_zero = True):\n",
    "    '''\n",
    "    gap definition need to be improved for Superconducting sample data \n",
    "    after Bias_mV_offset_avg_test \n",
    "    I_avg --> I_0 = bias_mV_0 \n",
    "    \n",
    "    output : I, dI/dV, LDOS_fb, SCgap_map,SCgap_pos, SCgap_neg\n",
    "    find SCgap : based on plateau finding --> plateau map + ZBCP map \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    # get plateau area \n",
    "    # tolerance for I & LIX\n",
    "    \n",
    "    xr_data_prcssd = xr_data.copy(deep = True)\n",
    "                   \n",
    "    xr_data_prcssd['dIdV'] = xr_data_prcssd.I_fb.differentiate(\n",
    "        coord = 'bias_mV')\n",
    "    # numerically calculated dI/dV from I_fb\n",
    "    LIX_ratio = xr_data_prcssd.dIdV / xr_data_prcssd.LIX_fb\n",
    "       \n",
    "    xr_data_prcssd['LIX_unit_calc'] = np.abs(\n",
    "        LIX_ratio.mean())*xr_data_prcssd.LIX_fb\n",
    "    # LIX unit calibration \n",
    "    # pA unit : lock-in result \n",
    "    # LIX_unit_calc : calibrated as [A/V] unit for dI/dV\n",
    "       \n",
    "    \n",
    "    print('Find plateau in I &LIX each points')\n",
    "    if apply_SGfilter == True :\n",
    "        print('import savgolFilter_xr in advance' )\n",
    "        xr_data_sg = savgolFilter_xr(xr_data_prcssd, \n",
    "                                     window_length = window_length,\n",
    "                                     polyorder = polyorder)\n",
    "\n",
    "    else : \n",
    "        print ('without SavgolFilter_xr, check outliers')\n",
    "        xr_data_sg = xr_data_prcssd\n",
    "\n",
    "    if 'I_fb' in xr_data_prcssd.data_vars : \n",
    "        I_fb_plateau = abs(xr_data_sg['I_fb']) <= tolerance_I \n",
    "    else :\n",
    "        I_fb_plateau = abs(xr_data_sg['LIX_fb']) <= tolerance_LIx  \n",
    "        print ('No I_fb channel, use LIX instead')\n",
    "\n",
    "    if 'LIX_unit_calc' in xr_data_prcssd.data_vars : \n",
    "        LIX_fb_plateau = abs(xr_data_sg['LIX_unit_calc']) <= tolerance_LIX * np.abs( LIX_ratio.mean())\n",
    "    else: \n",
    "        LIX_fb_plateau = abs(xr_data_sg['LIX_fb']) <= tolerance_LIX \n",
    "        print ('test_ No LIX_unit_calc channel, use LIX instead for tolerance_LIX check-up')\n",
    "\n",
    "    I_LIX_plateau = I_fb_plateau*LIX_fb_plateau\n",
    "    # pixels in X,Y, bias_mV  intersection of plateau\n",
    "\n",
    "    xr_data_sg['I_LIX_plateau']=I_LIX_plateau\n",
    "    #I_LIX_plateau is where  plateau within I & LIX tolerance \n",
    "    # I tolerance is near Zero Current \n",
    "    # LIX tolerance is more flat area with in I tolerance area \n",
    "    # Energy gap near Zero bias  \n",
    "    \n",
    "    \n",
    "    ################################################\n",
    "    # adjust bias_mV at zero first\n",
    "    ####################################################\n",
    "    if bias_mV_set_zero == True:\n",
    "        # select I_LIX_plateau is False ==> non-zero conductance at zero biase) \n",
    "        # apply boolean to I_fb & areal average \n",
    "        # find base at Zero Current \n",
    "\n",
    "        non_zero_condunctance_avg  = xr_data_sg.I_fb.where(~xr_data_sg.I_LIX_plateau.sel(bias_mV=0, method='nearest')).mean(dim = ['X','Y'])\n",
    "        # find bias_mV value in where the close to zero current \n",
    "        # xr_data_prcssd.bias_mV[np.abs(non_zero_condunctance_avg).argmin()]\n",
    "        if non_zero_condunctance_avg.sum() ==0 : \n",
    "            pass\n",
    "        else:\n",
    "            # error message with \"All-NaN slice encountered\"\n",
    "            #bias_mV_shift = grid_3D_gap.bias_mV -  grid_3D_gap.bias_mV[np.abs(non_zero_condunctance_avg).argmin()]\n",
    "            print(\"bias_mV zero where I_fb =0\" , xr_data_sg.bias_mV[np.abs(non_zero_condunctance_avg).argmin()].values)\n",
    "            # use assign_coords to change bias_mV values \n",
    "            xr_data_prcssd = xr_data_sg.assign_coords (bias_mV = xr_data_sg.bias_mV -  xr_data_sg.bias_mV[np.abs(non_zero_condunctance_avg).argmin()])\n",
    "            print(\"zero bias_mV: shifted\")\n",
    "    else: pass\n",
    "    \n",
    "    \n",
    "    ##################################################\n",
    "    ### find gap position again after bias_mV adjusted \n",
    "    #####################################################\n",
    "    \n",
    "    if 'I_fb' in xr_data_prcssd.data_vars : \n",
    "        I_fb_plateau = abs(xr_data_prcssd['I_fb']) <= tolerance_I \n",
    "    else :\n",
    "        I_fb_plateau = abs(xr_data_prcssd['LIX_fb']) <= tolerance_LIx  \n",
    "        print ('No I_fb channel, use LIX instead')\n",
    "\n",
    "    if 'LIX_unit_calc' in xr_data_prcssd.data_vars : \n",
    "        LIX_fb_plateau = abs(xr_data_prcssd['LIX_unit_calc']) <= tolerance_LIX *np.abs( LIX_ratio.mean())\n",
    "    else: \n",
    "        LIX_fb_plateau = abs(xr_data_prcssd['LIX_fb']) <= tolerance_LIX \n",
    "        print ('No LIX_unit_calc channel, use LIX instead for tolerance_LIX check-up')\n",
    "\n",
    "    I_LIX_plateau = I_fb_plateau*LIX_fb_plateau\n",
    "    # pixels in X,Y, bias_mV  intersection of plateau\n",
    "\n",
    "    xr_data_prcssd['I_LIX_plateau'] = I_LIX_plateau\n",
    "    \n",
    "    \n",
    "    # out figure\n",
    "    gap_pos0_I = xr_data_prcssd.I_fb.where(I_LIX_plateau).idxmax(dim='bias_mV')\n",
    "    gap_neg0_I = xr_data_prcssd.I_fb.where(I_LIX_plateau).idxmin(dim='bias_mV')\n",
    "    gap_mapI = gap_pos0_I-gap_neg0_I\n",
    "    \n",
    "    \n",
    "    \n",
    "    xr_data_prcssd['gap_pos0_I'] = gap_pos0_I\n",
    "    xr_data_prcssd['gap_neg0_I'] = gap_neg0_I\n",
    "    xr_data_prcssd['gap_mapI'] = gap_mapI\n",
    "    #########\n",
    "    \n",
    "    gap_pos0_LIX_mV = xr_data_prcssd.LIX_unit_calc.where(I_LIX_plateau).idxmax(dim='bias_mV')\n",
    "    gap_neg0_LIX_mV = xr_data_prcssd.LIX_unit_calc.where(I_LIX_plateau).idxmin(dim='bias_mV')\n",
    "   \n",
    "    # I_LIX_plateau  가운데  max min 을 골라냈음. (전체가운데 0가 포함하는지는 아직 모름. \n",
    "    \n",
    "    \n",
    "    xr_data_prcssd['gap_pos0_LIX'] = gap_pos0_LIX_mV\n",
    "    xr_data_prcssd['gap_neg0_LIX'] = gap_neg0_LIX_mV\n",
    "    \n",
    "    #######################################################\n",
    "    # filtering gap_pos0_LIX <--- filtering 'neg' values \n",
    "    # filtering gap_neg0_LIX <--- filtering 'pos' values \n",
    "    #########\n",
    "    #gap_neg0_LIX_neg = xr_data_prcssd.gap_neg0_LIX.where(xr_data_prcssd.gap_neg0_LIX>0).isnull()\n",
    "    # True ==>   neg == neg\n",
    "    gap_neg0_LIX_neg = xr_data_prcssd.gap_neg0_LIX.where(gap_neg0_LIX_mV<0)\n",
    "    xr_data_prcssd['gap_neg0_LIX']= gap_neg0_LIX_neg\n",
    "    # assign again \n",
    "    \n",
    "    \n",
    "    #gap_pos0_LIX_pos = xr_data_prcssd.gap_pos0_LIX.where(xr_data_prcssd.gap_pos0_LIX<0).isnull()\n",
    "    # True ==>  pos == pos\n",
    "    gap_pos0_LIX_pos = xr_data_prcssd.gap_pos0_LIX.where(xr_data_prcssd.gap_pos0_LIX>0)\n",
    "    xr_data_prcssd['gap_pos0_LIX']=gap_pos0_LIX_pos\n",
    "    # assign again \n",
    "    \n",
    "    \n",
    "    plateau_map_LIX = (~gap_pos0_LIX_pos.isnull())&(~gap_neg0_LIX_neg.isnull())\n",
    "    #     plateau_map_LIX = gap_neg0_LIX_neg & gap_pos0_LIX_pos \n",
    "    \n",
    "    \n",
    "    # select plateau that contains ZeroBias  ---> plateau_map (zero LIX at zero bias) \n",
    "    xr_data_prcssd['plateau_map_LIX'] = plateau_map_LIX\n",
    "    plateau_pos0_LIX = xr_data_prcssd.LIX_unit_calc.where(plateau_map_LIX).idxmax(dim='bias_mV')\n",
    "    plateau_neg0_LIX = xr_data_prcssd.LIX_unit_calc.where(plateau_map_LIX).idxmin(dim='bias_mV')\n",
    "    # LIX plateau area min & max \n",
    "    #xr_data_prcssd['plateau_pos0_LIX'] = plateau_pos0_LIX\n",
    "    #xr_data_prcssd['plateau_neg0_LIX'] = plateau_neg0_LIX\n",
    "    \n",
    "    xr_data_prcssd['plateau_size_map_LIX'] = gap_pos0_LIX_pos-gap_neg0_LIX_neg\n",
    "    # plateau_size_map_LIX\n",
    "    xr_data_prcssd['zerobiasconductance'] = xr_data_prcssd.where(~plateau_map_LIX).LIX_unit_calc.sel(bias_mV=0, method = 'nearest')\n",
    "    # non zero LIX area zerobias conductance map \n",
    "    \n",
    "    #gap_map_LIX = gap_pos0_LIX.where(grid_3D_gap.gap_neg0_LIX>0) - gap_neg0_LIX.where(grid_3D_gap.gap_neg0_LIX<0)\n",
    "    \n",
    "    ###############################################\n",
    "    # in case of  LIX offset (due to phase mismatching?) \n",
    "    \"\"\"\n",
    "    # LIX_fb values less than LIX_min_A => zero value \n",
    "    # LIX_0_pA = LIX_0_pA\n",
    "    LIX_0_AV  =  LIX_0_pA * LIX_ratio.mean()\n",
    "    # calibrated LIX resolution limit\n",
    "    gap_mask_LIX  = np.abs(grid_3D.LIX_unit_calc) < LIX_0_AV\n",
    "    # gap_mask_LIX  = np.abs(grid_3D.LIX_fb) > LIX_0_pA\n",
    "    # because of the same coefficient ('LIX_ratio.mean()')\n",
    "    # range for CBM &VBM is not different between  LIX_unit_calc & LIX_fb\n",
    "    # 3D mask \n",
    "\n",
    "    LIX_unit_calc_offst = grid_3D.dIdV.where(gap_mask_I).mean()- grid_3D['LIX_unit_calc'].where(gap_mask_I).mean()\n",
    "    # possible LIX offset adjust (based on dI/dV calc value)\n",
    "    grid_3D_prcssd['LDOS_fb'] = grid_3D.LIX_unit_calc + LIX_unit_calc_offst\n",
    "    # assign dI/dV value at I=0  as a reference offset \n",
    "    # grid_3D['LDOS_fb'] is calibrated dIdV with correct unit ([A/V]) for LDOS \n",
    "    # LDOS_fb is proportional to the real LDOS\n",
    "    # here we dont consider the matrix element for\n",
    "    \"\"\"\n",
    "    \n",
    "    #xr_data_prcssd = xr_data_prcssd.drop('gap_pos0_LIX')\n",
    "    #xr_data_prcssd = xr_data_prcssd.drop('gap_neg0_LIX')\n",
    "    \n",
    "    xr_data_prcssd.attrs['I[A]_limit'] = tolerance_I\n",
    "    xr_data_prcssd.attrs['LDOS[A/V]_limit'] = tolerance_LIX\n",
    "    xr_data_prcssd['LDOS_fb'] = xr_data_prcssd['LIX_unit_calc']\n",
    "    # meaningless redundant channel name. \n",
    "    # save the LDOS_fb for other functions. \n",
    "    \n",
    "    \n",
    "    return xr_data_prcssd\n",
    "\n",
    "#test\n",
    "#grid_3D_gap = grid_3D_Gap(grid_3D)\n",
    "#grid_3D_gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82bd5e47-62d7-4c86-8aa3-95c932112b0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hv_bias_mV_slicing(xr_data,ch = 'LIX_fb',frame_width = 200,cmap = 'bwr'): \n",
    "    '''\n",
    "    input : xarray dataset \n",
    "    output : holoview image\n",
    "    \n",
    "    * slicing 3D data set in XY plane \n",
    "    * bias_mV is knob\n",
    "    \n",
    "    default channel  =  'LIX_fb',  or assgin 'I_fb' or 'LDOS_fb'\n",
    "    default setting for frame width and cmap  can be changed. \n",
    "    \n",
    "    if you need to add color limit \n",
    "        add \".opts(clim=(0, 1E-10))\"\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    import holoviews as hv\n",
    "    from holoviews import opts\n",
    "\n",
    "    xr_data_hv = hv.Dataset(xr_data[ch])\n",
    "\n",
    "    hv.extension('bokeh')\n",
    "    ###############\n",
    "    # bias_mV slicing\n",
    "    dmap_plane  = [\"X\",\"Y\"]\n",
    "    dmap = xr_data_hv.to(hv.Image,\n",
    "                         kdims = dmap_plane,\n",
    "                         dynamic = True )\n",
    "    dmap.opts(colorbar = True,\n",
    "              cmap = 'bwr',\n",
    "              frame_width = frame_width,\n",
    "              aspect = 'equal').relabel('XY plane slicing: ')\n",
    "    fig = hv.render(dmap)\n",
    "    return dmap   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da59eb26-92d5-4df5-aacc-609f17d50322",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hv_fft_bias_mV_slicing(xr_data,ch = 'LDOS_fb_fft',frame_width = 200,cmap = 'bwr'): \n",
    "    '''\n",
    "    input : xarray dataset \n",
    "    output : holoview image\n",
    "    \n",
    "    * slicing 3D data set in XY plane \n",
    "    * bias_mV is knob\n",
    "    \n",
    "    default channel  =  'LIX_fb',  or assgin 'I_fb' or 'LDOS_fb'\n",
    "    default setting for frame width and cmap  can be changed. \n",
    "    \n",
    "    if you need to add color limit \n",
    "        add \".opts(clim=(0, 1E-10))\"\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    import holoviews as hv\n",
    "    from holoviews import opts\n",
    "\n",
    "    xr_data_hv = hv.Dataset(xr_data[ch])\n",
    "\n",
    "    hv.extension('bokeh')\n",
    "    ###############\n",
    "    # bias_mV slicing\n",
    "    dmap_plane  = [\"freq_X\",\"freq_Y\"]\n",
    "    dmap = xr_data_hv.to(hv.Image,\n",
    "                         kdims = dmap_plane,\n",
    "                         dynamic = True )\n",
    "    dmap.opts(colorbar = True,\n",
    "              cmap = 'bwr',\n",
    "              frame_width = frame_width,\n",
    "              aspect = 'equal').relabel('XY plane slicing: ')\n",
    "    fig = hv.render(dmap)\n",
    "    return dmap   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14308f-7f0e-46f4-996b-f018d66e2462",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cd90fa-18fd-4dc4-848c-a4e69e2c0dca",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8497789b-2d6b-400a-8060-8f5ed9fb650b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hv_XY_slicing(xr_data,ch = 'LIX_fb', slicing= 'X', frame_width = 200,cmap = 'bwr'): \n",
    "    '''\n",
    "    input : xarray dataset \n",
    "    output : holoview image \n",
    "    \n",
    "    \n",
    "    * slicing 3D data set in X-bias_mV or Y-bias_mV plane \n",
    "    * X or Y position is knob\n",
    "    \n",
    "    \n",
    "    default channel  =  'LIX_fb',  or assgin 'I_fb'\n",
    "    default setting for frame width and cmap  can be changed. \n",
    "    if you need to add color limit \n",
    "     \n",
    "    add \".opts(clim=(0, 1E-10))\"\n",
    "    \n",
    "    '''\n",
    "    import holoviews as hv\n",
    "    from holoviews import opts\n",
    "\n",
    "    xr_data_hv = hv.Dataset(xr_data[ch])\n",
    "\n",
    "    hv.extension('bokeh')\n",
    "    ###############\n",
    "    # bias_mV slicing\n",
    "    if slicing == 'Y':\n",
    "        dmap_plane  = [ \"X\",\"bias_mV\"]\n",
    "\n",
    "        dmap = xr_data_hv.to(hv.Image,\n",
    "                             kdims = dmap_plane,\n",
    "                             dynamic = True )\n",
    "        dmap.opts(colorbar = True,\n",
    "                  cmap = 'bwr',\n",
    "                  frame_width = frame_width).relabel('X - bias_mV plane slicing: ')\n",
    "    else : #slicing= 'X'\n",
    "        dmap_plane  = [ \"Y\",\"bias_mV\"]\n",
    "\n",
    "        dmap = xr_data_hv.to(hv.Image,\n",
    "                             kdims = dmap_plane,\n",
    "                             dynamic = True )\n",
    "        dmap.opts(colorbar = True,\n",
    "                  cmap = 'bwr',\n",
    "                  frame_width = frame_width).relabel('Y - bias_mV plane slicing: ')\n",
    "    fig = hv.render(dmap)\n",
    "    return dmap   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "530baf87-5353-4ec9-8705-c13bfab70bdb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hv_fft_XY_slicing(xr_data,ch = 'LDOS_fb_fft', slicing= 'X', frame_width = 200,cmap = 'bwr'): \n",
    "    '''\n",
    "    input : xarray dataset \n",
    "    output : holoview image \n",
    "    \n",
    "    \n",
    "    * slicing 3D data set in X-bias_mV or Y-bias_mV plane \n",
    "    * X or Y position is knob\n",
    "    \n",
    "    \n",
    "    default channel  =  'LIX_fb',  or assgin 'I_fb'\n",
    "    default setting for frame width and cmap  can be changed. \n",
    "    if you need to add color limit \n",
    "     \n",
    "    add \".opts(clim=(0, 1E-10))\"\n",
    "    \n",
    "    '''\n",
    "    import holoviews as hv\n",
    "    from holoviews import opts\n",
    "\n",
    "    xr_data_hv = hv.Dataset(xr_data[ch])\n",
    "\n",
    "    hv.extension('bokeh')\n",
    "    ###############\n",
    "    # bias_mV slicing\n",
    "    if slicing == 'freq_Y':\n",
    "        dmap_plane  = [ \"freq_X\",\"freq_bias_mV\"]\n",
    "\n",
    "        dmap = xr_data_hv.to(hv.Image,\n",
    "                             kdims = dmap_plane,\n",
    "                             dynamic = True )\n",
    "        dmap.opts(colorbar = True,\n",
    "                  cmap = 'bwr',\n",
    "                  frame_width = frame_width).relabel('X - bias_mV plane slicing: ')\n",
    "    else : #slicing= 'freq_X'\n",
    "        dmap_plane  = [ \"freq_Y\",\"freq_bias_mV\"]\n",
    "\n",
    "        dmap = xr_data_hv.to(hv.Image,\n",
    "                             kdims = dmap_plane,\n",
    "                             dynamic = True )\n",
    "        dmap.opts(colorbar = True,\n",
    "                  cmap = 'bwr',\n",
    "                  frame_width = frame_width).relabel('Y - bias_mV plane slicing: ')\n",
    "    fig = hv.render(dmap)\n",
    "    return dmap   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e1d47b-bcc1-4eba-9e91-0e07e01d5bd3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_XYslice_w_LDOS (xr_data, data_channel='LIX_fb', slicing_bias_mV = 2):\n",
    "    \n",
    "    '''\n",
    "    ################################\n",
    "    # use the slider in advance \n",
    "    sliderX = pnw.IntSlider(name='X', \n",
    "                           start = 0 ,\n",
    "                           end = grid_3D.X.shape[0]) \n",
    "    sliderY = pnw.IntSlider(name='Y', \n",
    "                           start = 0 ,\n",
    "                           end = grid_3D.Y.shape[0]) \n",
    "\n",
    "    #sliderX_v_intact = interact(lambda x:  grid_3D.X[x].values, x =sliderX)[1]\n",
    "    #sliderY_v_intact = interact(lambda y:  grid_3D.Y[y].values, y =sliderY)[1]\n",
    "    pn.Column(interact(lambda x:  grid_3D.X[x].values, x =sliderX), interact(lambda y: grid_3D.Y[y].values, y =sliderY))\n",
    "\n",
    "    ####################################\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print(\"use the sliderX&Y first\")\n",
    "    plt.style.use('default')\n",
    "    sliderX_v = xr_data.X[sliderX.value].values\n",
    "    sliderY_v = xr_data.Y[sliderY.value].values\n",
    "\n",
    "\n",
    "    xr_data_Hline_profile = xr_data.isel(Y = sliderY.value)[data_channel]\n",
    "\n",
    "    xr_data_Vline_profile = xr_data.isel(X = sliderX.value)[data_channel]\n",
    "    \n",
    "    # bias_mV slicing\n",
    "    fig,axes = plt.subplots (nrows = 2,\n",
    "                            ncols = 2,\n",
    "                            figsize = (6,6))\n",
    "    axs = axes.ravel()\n",
    "\n",
    "    isns.imshow(xr_data.LIX_fb.sel(bias_mV = slicing_bias_mV, method=\"nearest\" ),\n",
    "                    ax =  axs[0],\n",
    "                    robust = True)\n",
    "    axs[0].hlines(sliderY.value,0,xr_data.X.shape[0], lw = 1, color = 'c')\n",
    "    axs[0].vlines(sliderX.value,0,xr_data.Y.shape[0], lw = 1, color = 'm')    \n",
    "\n",
    "    xr_data_Vline_profile.plot(ax = axs[1],robust = True)#, vmin = xr_data_Vline_profile.to_numpy().min() , vmax = xr_data_Vline_profile.to_numpy().max())\n",
    "    xr_data_Hline_profile.T.plot(ax = axs[2],robust = True)#, vmin = xr_data_Hline_profile.to_numpy().min() , vmax = xr_data_Hline_profile.to_numpy().max())\n",
    "\n",
    "    xr_data.LIX_fb.isel(X =sliderX.value, Y =sliderY.value) .plot(ax =axs[3])\n",
    "    #pn.Row(pn.Column(dmap_slideXY,xr_data_Vline_profile.plot()), )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07076a14-143c-4f90-b98e-4f7ff3402706",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_Xslice_w_LDOS (xr_data, sliderX, ch ='LIX_fb', slicing_bias_mV = 0):\n",
    "    \n",
    "    '''\n",
    "    ################################\n",
    "    # use the slider in advance \n",
    "    sliderX = pnw.IntSlider(name='X', \n",
    "                           start = 0 ,\n",
    "                           end = grid_3D.X.shape[0]) \n",
    "    sliderY = pnw.IntSlider(name='Y', \n",
    "                           start = 0 ,\n",
    "                           end = grid_3D.Y.shape[0]) \n",
    "\n",
    "    #sliderX_v_intact = interact(lambda x:  grid_3D.X[x].values, x =sliderX)[1]\n",
    "    #sliderY_v_intact = interact(lambda y:  grid_3D.Y[y].values, y =sliderY)[1]\n",
    "    pn.Column(interact(lambda x:  grid_3D.X[x].values, x =sliderX), interact(lambda y: grid_3D.Y[y].values, y =sliderY))\n",
    "\n",
    "    ####################################\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print(\"use the sliderX&Y first\")\n",
    "    #plt.style.use('default')\n",
    "    sliderX_v = xr_data.X[sliderX.value].values\n",
    "    sliderY_v = xr_data.Y[sliderY.value].values\n",
    "\n",
    "\n",
    "    xr_data_Hline_profile = xr_data.isel(Y = sliderY.value)[ch]\n",
    "\n",
    "    xr_data_Vline_profile = xr_data.isel(X = sliderX.value)[ch]\n",
    "    \n",
    "    # bias_mV slicing\n",
    "    fig,axes = plt.subplots (nrows = 2,\n",
    "                            ncols = 1,\n",
    "                            figsize = (3,6))\n",
    "    axs = axes.ravel()\n",
    "\n",
    "    isns.imshow(xr_data[ch].sel(bias_mV = slicing_bias_mV, method=\"nearest\" ),\n",
    "                    ax =  axs[0],\n",
    "                    robust = True)\n",
    "    axs[0].hlines(sliderY.value,0,xr_data.X.shape[0], lw = 1, color = 'c')\n",
    "    axs[0].vlines(sliderX.value,0,xr_data.Y.shape[0], lw = 1, color = 'm')    \n",
    "\n",
    "    xr_data_Vline_profile.plot(ax = axs[1], robust = True, vmin = xr_data_Vline_profile.to_numpy().min() , vmax = xr_data_Vline_profile.to_numpy().max()*0.3)\n",
    "    #xr_data_Hline_profile.T.plot(ax = axs[2], robust = True)#, vmin = xr_data_Hline_profile.to_numpy().min() , vmax = xr_data_Hline_profile.to_numpy().max())\n",
    "    axs[1].vlines(0,0,xr_data.Y.shape[0], lw = 1, color = 'w',ls ='--', alpha =0.3) \n",
    "    #xr_data[ch].isel(X =sliderX.value, Y =sliderY.value) .plot(ax =axs[2])\n",
    "    #pn.Row(pn.Column(dmap_slideXY,xr_data_Vline_profile.plot()), )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e97dabf-07d9-4ca9-ae35-f1384ffeed4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_Xslice_w_LDOS (xr_data, sliderX, ch ='LIX_fb', slicing_bias_mV = 0):\n",
    "    \n",
    "    '''\n",
    "    ################################\n",
    "    # use the slider in advance \n",
    "    sliderX = pnw.IntSlider(name='X', \n",
    "                           start = 0 ,\n",
    "                           end = grid_3D.X.shape[0]) \n",
    "    sliderY = pnw.IntSlider(name='Y', \n",
    "                           start = 0 ,\n",
    "                           end = grid_3D.Y.shape[0]) \n",
    "\n",
    "    #sliderX_v_intact = interact(lambda x:  grid_3D.X[x].values, x =sliderX)[1]\n",
    "    #sliderY_v_intact = interact(lambda y:  grid_3D.Y[y].values, y =sliderY)[1]\n",
    "    pn.Column(interact(lambda x:  grid_3D.X[x].values, x =sliderX), interact(lambda y: grid_3D.Y[y].values, y =sliderY))\n",
    "\n",
    "    ####################################\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print(\"use the sliderX&Y first\")\n",
    "    #plt.style.use('default')\n",
    "    sliderX_v = xr_data.X[sliderX.value].values\n",
    "    sliderY_v = xr_data.Y[sliderY.value].values\n",
    "\n",
    "\n",
    "    xr_data_Hline_profile = xr_data.isel(Y = sliderY.value)[ch]\n",
    "\n",
    "    xr_data_Vline_profile = xr_data.isel(X = sliderX.value)[ch]\n",
    "    \n",
    "    # bias_mV slicing\n",
    "    fig,axes = plt.subplots (nrows = 2,\n",
    "                            ncols = 1,\n",
    "                            figsize = (3,6))\n",
    "    axs = axes.ravel()\n",
    "\n",
    "    isns.imshow(xr_data[ch].sel(bias_mV = slicing_bias_mV, method=\"nearest\" ),\n",
    "                    ax =  axs[0],\n",
    "                    robust = True)\n",
    "    axs[0].hlines(sliderY.value,0,xr_data.X.shape[0], lw = 1, color = 'c')\n",
    "    axs[0].vlines(sliderX.value,0,xr_data.Y.shape[0], lw = 1, color = 'm')    \n",
    "\n",
    "    xr_data_Vline_profile.plot(ax = axs[1], robust = True, vmin = xr_data_Vline_profile.to_numpy().min(), vmax = xr_data_Vline_profile.to_numpy().max()*0.25)\n",
    "    #xr_data_Hline_profile.T.plot(ax = axs[2], robust = True)#, vmin = xr_data_Hline_profile.to_numpy().min() , vmax = xr_data_Hline_profile.to_numpy().max())\n",
    "    axs[1].vlines(0,0,xr_data.Y.shape[0], lw = 1, color = 'w',ls ='--', alpha =0.3) \n",
    "    # L half alpha\n",
    "    axs[1].vlines(0.368181818,0,xr_data.Y.shape[0], lw = 1, color = 'b',ls =':', alpha =0.3) \n",
    "    axs[1].vlines(1.104545455,0,xr_data.Y.shape[0], lw = 1, color = 'b',ls =':', alpha =0.3)     \n",
    "    axs[1].vlines(1.840909091,0,xr_data.Y.shape[0], lw = 1, color = 'b',ls =':', alpha =0.3) \n",
    "    axs[1].vlines(-0.368181818,0,xr_data.Y.shape[0], lw = 1, color = 'b',ls =':', alpha =0.3) \n",
    "    axs[1].vlines(-1.104545455,0,xr_data.Y.shape[0], lw = 1, color = 'b',ls =':', alpha =0.3)     \n",
    "    axs[1].vlines(-1.840909091,0,xr_data.Y.shape[0], lw = 1, color = 'b',ls =':', alpha =0.3) \n",
    "    \n",
    "    # L int  alpha\n",
    "    0.736363636, 1.472727273, 2.209090909\n",
    "    axs[1].vlines(0.736363636,0,xr_data.Y.shape[0], lw = 1, color = 'r',ls ='--', alpha =0.3) \n",
    "    axs[1].vlines(1.472727273,0,xr_data.Y.shape[0], lw = 1, color = 'r',ls ='--', alpha =0.3)     \n",
    "    axs[1].vlines(1.840909091,0,xr_data.Y.shape[0], lw = 1, color = 'r',ls ='--', alpha =0.3) \n",
    "    axs[1].vlines(-0.736363636,0,xr_data.Y.shape[0], lw = 1, color = 'r',ls ='--', alpha =0.3) \n",
    "    axs[1].vlines(-1.472727273,0,xr_data.Y.shape[0], lw = 1, color = 'r',ls ='--', alpha =0.3)     \n",
    "    axs[1].vlines(-2.2090909091,0,xr_data.Y.shape[0], lw = 1, color = 'r',ls ='--', alpha =0.3) \n",
    "    \n",
    "    #xr_data[ch].isel(X =sliderX.value, Y =sliderY.value) .plot(ax =axs[2])\n",
    "    #pn.Row(pn.Column(dmap_slideXY,xr_data_Vline_profile.plot()), )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ae91297-544e-4623-8c42-295ff1a2d6b0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c13b980-d846-4f34-ba43-b5fe02df962d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_Yslice_w_LDOS (xr_data, sliderY, ch ='LIX_fb', slicing_bias_mV = 0):\n",
    "    \n",
    "    '''\n",
    "    ################################\n",
    "    # use the slider in advance \n",
    "    sliderX = pnw.IntSlider(name='X', \n",
    "                           start = 0 ,\n",
    "                           end = grid_3D.X.shape[0]) \n",
    "    sliderY = pnw.IntSlider(name='Y', \n",
    "                           start = 0 ,\n",
    "                           end = grid_3D.Y.shape[0]) \n",
    "\n",
    "    #sliderX_v_intact = interact(lambda x:  grid_3D.X[x].values, x =sliderX)[1]\n",
    "    #sliderY_v_intact = interact(lambda y:  grid_3D.Y[y].values, y =sliderY)[1]\n",
    "    pn.Column(interact(lambda x:  grid_3D.X[x].values, x =sliderX), interact(lambda y: grid_3D.Y[y].values, y =sliderY))\n",
    "\n",
    "    ####################################\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print(\"use the sliderX&Y first\")\n",
    "    #plt.style.use('default')\n",
    "    #sliderX_v = xr_data.X[sliderX.value].values\n",
    "    sliderY_v = xr_data.Y[sliderY.value].values\n",
    "\n",
    "\n",
    "    xr_data_Hline_profile = xr_data.isel(Y = sliderY.value)[ch]\n",
    "\n",
    "    #xr_data_Vline_profile = xr_data.isel(X = sliderX.value)[ch]\n",
    "    \n",
    "    # bias_mV slicing\n",
    "    fig,axes = plt.subplots (nrows = 1,\n",
    "                            ncols = 2,\n",
    "                            figsize = (6,3))\n",
    "    axs = axes.ravel()\n",
    "\n",
    "    isns.imshow(xr_data[ch].sel(bias_mV = slicing_bias_mV, method=\"nearest\" ).T,\n",
    "                    ax =  axs[0],\n",
    "                    robust = True)\n",
    "    axs[0].vlines(sliderY.value,0,xr_data.X.shape[0], lw = 1, color = 'c')\n",
    "    #axs[0].vlines(sliderX.value,0,xr_data.Y.shape[0], lw = 1, color = 'm')    \n",
    "\n",
    "    #xr_data_Vline_profile.plot(ax = axs[1], robust = True, vmin = xr_data_Vline_profile.to_numpy().min(), vmax = xr_data_Vline_profile.to_numpy().max()*0.25)\n",
    "    #xr_data_Hline_profile.plot(ax = axs[1], robust = True)#, vmin = xr_data_Hline_profile.to_numpy().min() , vmax = xr_data_Hline_profile.to_numpy().max())\n",
    "    xr_data_Hline_profile.plot(ax = axs[1], robust = True, vmin = xr_data_Hline_profile.to_numpy().min() , vmax = xr_data_Hline_profile.to_numpy().max()*0.20)\n",
    "    axs[1].vlines(0,0,xr_data.Y.shape[0], lw = 1, color = 'w',ls ='--', alpha =0.3) \n",
    "    # L half alpha\n",
    "    axs[1].vlines(0.368181818,0,xr_data.Y.shape[0], lw = 1, color = 'w',ls =':', alpha =0.3) \n",
    "    axs[1].vlines(1.104545455,0,xr_data.Y.shape[0], lw = 1, color = 'w',ls =':', alpha =0.3)     \n",
    "    axs[1].vlines(1.840909091,0,xr_data.Y.shape[0], lw = 1, color = 'w',ls =':', alpha =0.3) \n",
    "    axs[1].vlines(-0.368181818,0,xr_data.Y.shape[0], lw = 1, color = 'w',ls =':', alpha =0.3) \n",
    "    axs[1].vlines(-1.104545455,0,xr_data.Y.shape[0], lw = 1, color = 'w',ls =':', alpha =0.3)     \n",
    "    axs[1].vlines(-1.840909091,0,xr_data.Y.shape[0], lw = 1, color = 'w',ls =':', alpha =0.3) \n",
    "    \n",
    "    # L int  alpha\n",
    "    0.736363636, 1.472727273, 2.209090909\n",
    "    axs[1].vlines(0.736363636,0,xr_data.Y.shape[0], lw = 1, color = 'r',ls ='-', alpha =0.5) \n",
    "    axs[1].vlines(1.472727273,0,xr_data.Y.shape[0], lw = 1, color = 'r',ls ='-', alpha =0.5)     \n",
    "    axs[1].vlines(1.840909091,0,xr_data.Y.shape[0], lw = 1, color = 'r',ls ='-', alpha =0.5) \n",
    "    axs[1].vlines(-0.736363636,0,xr_data.Y.shape[0], lw = 1, color = 'r',ls ='-', alpha =0.5) \n",
    "    axs[1].vlines(-1.472727273,0,xr_data.Y.shape[0], lw = 1, color = 'r',ls ='-', alpha =0.5)     \n",
    "    axs[1].vlines(-2.2090909091,0,xr_data.Y.shape[0], lw = 1, color = 'r',ls ='-', alpha =0.5) \n",
    "    \n",
    "    #xr_data[ch].isel(X =sliderX.value, Y =sliderY.value) .plot(ax =axs[2])\n",
    "    #pn.Row(pn.Column(dmap_slideXY,xr_data_Vline_profile.plot()), )\n",
    "    \n",
    "    \n",
    "    axs[1].set_xlim(-2.5,2.5)\n",
    "    # set x range limit\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc95f688-38d3-446b-881f-d3ef869c37c9",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function for drawing bbox averaged STS \n",
    "# only after bbox setup & streaming bound_box positions\n",
    "\n",
    "\n",
    "def hv_bbox_topo_avg (xr_data, bound_box , ch = 'topography' ):\n",
    "    '''\n",
    "    ** only after Bound box settup with hV \n",
    "    \n",
    "        import holoviews as hv\n",
    "        from holoviews import opts\n",
    "        hv.extension('bokeh')\n",
    "\n",
    "        grid_channel_hv = hv.Dataset(grid_3D.I_fb)\n",
    "\n",
    "        # bias_mV slicing\n",
    "        dmap_plane  = [\"X\",\"Y\"]\n",
    "        dmap = grid_channel_hv.to(hv.Image,\n",
    "                                  kdims = dmap_plane,\n",
    "                                  dynamic = True )\n",
    "        dmap.opts(colorbar = True,\n",
    "                  cmap = 'bwr',\n",
    "                  frame_width = 200,\n",
    "                  aspect = 'equal')#.relabel('XY plane slicing: ')\n",
    "\n",
    "        grid_channel_hv_image  = hv.Dataset(grid_3D.I_fb.isel(bias_mV = 0)).relabel('for BBox selection : ')\n",
    "\n",
    "        bbox_points = hv.Points(grid_channel_hv_image).opts(frame_width = 200,\n",
    "                                                            color = 'k',\n",
    "                                                            aspect = 'equal',\n",
    "                                                            alpha = 0.1,                                   \n",
    "                                                            tools=['box_select'])\n",
    "\n",
    "        bound_box = hv.streams.BoundsXY(source = bbox_points,\n",
    "                                        bounds=(0,0,0,0))\n",
    "        dmap*bbox_points\n",
    "        \n",
    "        add grid_topo line profile \n",
    "\n",
    "    \n",
    "    '''\n",
    "    import holoviews as hv\n",
    "    from holoviews import opts\n",
    "    hv.extension('bokeh')\n",
    "    # slicing bias_mV = 5 mV\n",
    "    \n",
    "    #bound_box.bounds\n",
    "    x_bounds_msk = (xr_data.X > bound_box.bounds[0] ) & (xr_data.X < bound_box.bounds[2])\n",
    "    y_bounds_msk = (xr_data.Y > bound_box.bounds[1] ) & (xr_data.Y < bound_box.bounds[3])\n",
    "\n",
    "    xr_data_bbox = xr_data.where (xr_data.X[x_bounds_msk] + xr_data.Y[y_bounds_msk])\n",
    "    \n",
    "    isns.reset_defaults()\n",
    "    isns.set_image(cmap=\"inferno\",origin = 'lower')\n",
    "    # isns image directino setting \n",
    "\n",
    "    fig,axs = plt.subplots (nrows = 1,\n",
    "                            ncols = 3,\n",
    "                            figsize = (12,4))\n",
    "\n",
    "    isns.imshow(xr_data[ch].values,\n",
    "                ax =  axs[0],\n",
    "                robust = True)\n",
    "\n",
    "    # add rectangle for bbox \n",
    "    from matplotlib.patches import Rectangle\n",
    "    # find index value of bound box \n",
    "\n",
    "    Bbox_x0 = np.abs((xr_data.X-bound_box.bounds[0]).to_numpy()).argmin()\n",
    "    Bbox_y0 = np.abs((xr_data.Y-bound_box.bounds[1]).to_numpy()).argmin()\n",
    "    Bbox_x1 = np.abs((xr_data.X-bound_box.bounds[2]).to_numpy()).argmin()\n",
    "    Bbox_y1 = np.abs((xr_data.Y-bound_box.bounds[3]).to_numpy()).argmin()\n",
    "    Bbox = Bbox_x0,Bbox_y0,Bbox_x1,Bbox_y1\n",
    "    # substract value, absolute value with numpy, argmin returns index value\n",
    "\n",
    "    # when add rectangle, add_patch used index \n",
    "    axs[0].add_patch(Rectangle((Bbox_x0 , Bbox_y0 ), \n",
    "                               Bbox_x1 -Bbox_x0 , Bbox_y1-Bbox_y0,\n",
    "                               edgecolor = 'pink',\n",
    "                               fill=False,\n",
    "                               lw=2,\n",
    "                               alpha=0.5))\n",
    "\n",
    "    isns.imshow(xr_data_bbox[ch].values,\n",
    "                ax =  axs[1],\n",
    "                robust = True)\n",
    "\n",
    "    # topography along longer axis \n",
    "    if xr_data_bbox.X.size > xr_data_bbox.Y.size : \n",
    "        avg_dim =[ 'Y']\n",
    "    else : \n",
    "        avg_dim =[ 'X']\n",
    "\n",
    "    sns.lineplot(xr_data_bbox.mean(dim = avg_dim).to_dataframe(),\n",
    "                 ax = axs[2])\n",
    "    #plt.savefig('grid011_bbox)p.png')\n",
    "    plt.show()\n",
    "    \n",
    "    # 3 figures will be diplayed, original image with Bbox area, BBox area zoom, BBox averaged STS\n",
    "    return xr_data_bbox, fig\n",
    "    # plot STS at the selected points \n",
    "    # use the seaborn (confident interval : 95%) \n",
    "    # sns is figure-level function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "158bbc2f-71cd-4f13-b678-8bbd15e3968a",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# function for drawing bbox averaged STS \n",
    "# only after bbox setup & streaming bound_box positions\n",
    "\n",
    "\n",
    "def hv_bbox_avg (xr_data, bound_box , ch = 'LIX_fb' ,slicing_bias_mV = 0.5, show_LDOS_avg = False ):\n",
    "    '''\n",
    "    ** only after Bound box settup with hV \n",
    "    \n",
    "        import holoviews as hv\n",
    "        from holoviews import opts\n",
    "        hv.extension('bokeh')\n",
    "\n",
    "        grid_channel_hv = hv.Dataset(grid_3D.I_fb)\n",
    "\n",
    "        # bias_mV slicing\n",
    "        dmap_plane  = [\"X\",\"Y\"]\n",
    "        dmap = grid_channel_hv.to(hv.Image,\n",
    "                                  kdims = dmap_plane,\n",
    "                                  dynamic = True )\n",
    "        dmap.opts(colorbar = True,\n",
    "                  cmap = 'bwr',\n",
    "                  frame_width = 200,\n",
    "                  aspect = 'equal')#.relabel('XY plane slicing: ')\n",
    "\n",
    "        grid_channel_hv_image  = hv.Dataset(grid_3D.I_fb.isel(bias_mV = 0)).relabel('for BBox selection : ')\n",
    "\n",
    "        bbox_points = hv.Points(grid_channel_hv_image).opts(frame_width = 200,\n",
    "                                                            color = 'k',\n",
    "                                                            aspect = 'equal',\n",
    "                                                            alpha = 0.1,                                   \n",
    "                                                            tools=['box_select'])\n",
    "\n",
    "        bound_box = hv.streams.BoundsXY(source = bbox_points,\n",
    "                                        bounds=(0,0,0,0))\n",
    "        dmap*bbox_points\n",
    "        \n",
    "        add grid_topo line profile \n",
    "\n",
    "    \n",
    "    '''\n",
    "    import holoviews as hv\n",
    "    from holoviews import opts\n",
    "    hv.extension('bokeh')\n",
    "    # slicing bias_mV = 5 mV\n",
    "    \n",
    "    #bound_box.bounds\n",
    "    x_bounds_msk = (xr_data.X > bound_box.bounds[0] ) & (xr_data.X < bound_box.bounds[2])\n",
    "    y_bounds_msk = (xr_data.Y > bound_box.bounds[1] ) & (xr_data.Y < bound_box.bounds[3])\n",
    "\n",
    "    xr_data_bbox = xr_data.where (xr_data.X[x_bounds_msk] + xr_data.Y[y_bounds_msk])\n",
    "    \n",
    "    isns.reset_defaults()\n",
    "    isns.set_image(cmap= 'viridis',origin = 'lower')\n",
    "    # isns image directino setting \n",
    "    if show_LDOS_avg == True :\n",
    "        ncols = 3\n",
    "        \n",
    "    else : \n",
    "        ncols = 2 \n",
    "        \n",
    "    \n",
    "    fig,axs = plt.subplots (nrows = 1,\n",
    "                            ncols = ncols,\n",
    "                            figsize = (12,4))\n",
    "\n",
    "    isns.imshow(xr_data[ch].sel(bias_mV = slicing_bias_mV, method=\"nearest\" ),\n",
    "                ax =  axs[0],\n",
    "                robust = True)\n",
    "\n",
    "    # add rectangle for bbox \n",
    "    from matplotlib.patches import Rectangle\n",
    "    # find index value of bound box \n",
    "\n",
    "    Bbox_x0 = np.abs((xr_data.X-bound_box.bounds[0]).to_numpy()).argmin()\n",
    "    Bbox_y0 = np.abs((xr_data.Y-bound_box.bounds[1]).to_numpy()).argmin()\n",
    "    Bbox_x1 = np.abs((xr_data.X-bound_box.bounds[2]).to_numpy()).argmin()\n",
    "    Bbox_y1 = np.abs((xr_data.Y-bound_box.bounds[3]).to_numpy()).argmin()\n",
    "    Bbox = Bbox_x0,Bbox_y0,Bbox_x1,Bbox_y1\n",
    "    # substract value, absolute value with numpy, argmin returns index value\n",
    "\n",
    "    # when add rectangle, add_patch used index \n",
    "    axs[0].add_patch(Rectangle((Bbox_x0 , Bbox_y0 ), \n",
    "                               Bbox_x1 -Bbox_x0 , Bbox_y1-Bbox_y0,\n",
    "                               edgecolor = 'pink',\n",
    "                               fill=False,\n",
    "                               lw=2,\n",
    "                               alpha=0.5))\n",
    "\n",
    "    isns.imshow(xr_data_bbox[ch].sel(bias_mV = slicing_bias_mV, method=\"nearest\" ),\n",
    "                ax =  axs[1],\n",
    "                robust = True)\n",
    "    if show_LDOS_avg == True :   \n",
    "        sns.lineplot(x = \"bias_mV\",\n",
    "                     y = ch, \n",
    "                     data = xr_data_bbox.to_dataframe(),\n",
    "                     ax = axs[2])\n",
    "    else : pass\n",
    "    #plt.savefig('grid011_bbox)p.png')\n",
    "    plt.show()\n",
    "    # 3 figures will be diplayed, original image with Bbox area, BBox area zoom, BBox averaged STS\n",
    "    return xr_data_bbox, fig\n",
    "    # plot STS at the selected points \n",
    "    # use the seaborn (confident interval : 95%) \n",
    "    # sns is figure-level function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f7bb9d-7449-40d2-88f0-1a061cc35239",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe952a49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d409266f-d117-4260-b6a5-843a9efecdb1",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Signal Treatments \n",
    "* Assume that the coords in Xarray are 'X\",'Y','bias_mV'\n",
    "* if the xr is 2D array ==> (X,bias_mV) or (Y, bias_mV) \n",
    "## Savatzky-Golay smoothig \n",
    "    * use the list comprehension for the sg-smoothing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04eb82bc-604a-4eed-be4d-e5f41573e93e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "def savgolFilter_xr(xrdata,window_length=7,polyorder=3): \n",
    "    # window_length = odd number\n",
    "    #import copy\n",
    "    #xrdata_prcssd = copy.deepcopy(xrdata)\n",
    "    xrdata_prcssd = xrdata.copy()\n",
    "    print('Apply a Savitzky-Golay filter to an xarray Dataset.')\n",
    "\n",
    "    for data_ch in xrdata:\n",
    "\n",
    "        if len(xrdata[data_ch].dims) == 2:\n",
    "            \n",
    "            if xrdata[data_ch].dims == ('Y', 'X'):\n",
    "                print('3D data')\n",
    "                pass\n",
    "                \n",
    "            else: # in case of X& bias_mV or Y  & bias_mV case\n",
    "\n",
    "                # smoothing filter only for the 3D data set\n",
    "                # ==> updaded \n",
    "                xrdata_prcssd[data_ch]\n",
    "                ### 2D data case \n",
    "                ### assume that coords are 'X','Y','bias_mV'\n",
    "                #### two case X,bias_mV or Y,bias_mV \n",
    "                if 'X' in xrdata[data_ch].dims :\n",
    "                    x_axis = xrdata.X.size # or xrdata.dims.mapping['X']\n",
    "                    # xrdata is X,bias_mV \n",
    "                    # use the isel(X = x) \n",
    "                    xrdata_prcssd[data_ch] = xr.DataArray (\n",
    "                        np.array (\n",
    "                            [sp.signal.savgol_filter(xrdata[data_ch].isel(X = x).values,\n",
    "                                                     window_length, \n",
    "                                                     polyorder , \n",
    "                                                     mode = 'nearest')\n",
    "                             for x in range(x_axis)]),\n",
    "                        dims = [\"X\", \"bias_mV\"],\n",
    "                        coords = {\"X\": xrdata.X,\n",
    "                                  \"bias_mV\": xrdata.bias_mV})\n",
    "                elif 'Y' in xrdata[data_ch].dims  :                # xrdata is XY,bias_mV                 # use the isel(Y = y) \n",
    "                    y_axis = xrdata.Y.size\n",
    "                    xrdata_prcssd[data_ch] = xr.DataArray (\n",
    "                        np.array (\n",
    "                            [sp.signal.savgol_filter(xrdata[data_ch].isel(Y = y).values,\n",
    "                                                     window_length, \n",
    "                                                     polyorder , \n",
    "                                                     mode = 'nearest')\n",
    "                             for y in range(y_axis) ]),\n",
    "                        dims = [\"Y\", \"bias_mV\"],\n",
    "                        coords = {\"Y\": xrdata.Y,\n",
    "                                  \"bias_mV\": xrdata.bias_mV}\n",
    "                    )\n",
    "                else: pass\n",
    "            \n",
    "        elif len(xrdata[data_ch].dims) == 3:\n",
    "            x_axis = xrdata.X.size # or xrdata.dims.mapping['X']\n",
    "            y_axis = xrdata.Y.size\n",
    "            print (data_ch)\n",
    "            xrdata_prcssd[data_ch] = xr.DataArray (\n",
    "                np.array ([\n",
    "                    sp.signal.savgol_filter(xrdata[data_ch].isel(X = x, Y = y).values,\n",
    "                                            window_length, \n",
    "                                            polyorder , \n",
    "                                            mode = 'nearest')\n",
    "                    for y in range(y_axis) \n",
    "                    for x in range(x_axis)\n",
    "                ] ).reshape(y_axis,x_axis, xrdata.bias_mV.size),\n",
    "                dims = [\"Y\", \"X\", \"bias_mV\"],\n",
    "                coords = {\"X\": xrdata.X,\n",
    "                          \"Y\": xrdata.Y,\n",
    "                          \"bias_mV\": xrdata.bias_mV}            )\n",
    "            # transpose np array to correct X&Y direction \n",
    "        else : pass\n",
    "    return xrdata_prcssd\n",
    "\n",
    "#grid_2D_sg = savgolFilter_xr(grid_2D)\n",
    "#grid_2D_sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2c9eb4-964e-4661-af3a-79faf097c247",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_XYslice_w_LDOS (xr_data, sliderX, sliderY, ch ='LIX_fb', slicing_bias_mV = 2):\n",
    "    \n",
    "    '''\n",
    "    ################################\n",
    "    # use the slider in advance \n",
    "    sliderX = pnw.IntSlider(name='X', \n",
    "                           start = 0 ,\n",
    "                           end = grid_3D.X.shape[0]) \n",
    "    sliderY = pnw.IntSlider(name='Y', \n",
    "                           start = 0 ,\n",
    "                           end = grid_3D.Y.shape[0]) \n",
    "\n",
    "    #sliderX_v_intact = interact(lambda x:  grid_3D.X[x].values, x =sliderX)[1]\n",
    "    #sliderY_v_intact = interact(lambda y:  grid_3D.Y[y].values, y =sliderY)[1]\n",
    "    pn.Column(interact(lambda x:  grid_3D.X[x].values, x =sliderX), interact(lambda y: grid_3D.Y[y].values, y =sliderY))\n",
    "\n",
    "    ####################################\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print(\"use the sliderX&Y first\")\n",
    "    #plt.style.use('default')\n",
    "    sliderX_v = xr_data.X[sliderX.value].values\n",
    "    sliderY_v = xr_data.Y[sliderY.value].values\n",
    "\n",
    "\n",
    "    xr_data_Hline_profile = xr_data.isel(Y = sliderY.value)[ch]\n",
    "\n",
    "    xr_data_Vline_profile = xr_data.isel(X = sliderX.value)[ch]\n",
    "    \n",
    "    # bias_mV slicing\n",
    "    fig,axes = plt.subplots (nrows = 2,\n",
    "                            ncols = 2,\n",
    "                            figsize = (6,6))\n",
    "    axs = axes.ravel()\n",
    "\n",
    "    isns.imshow(xr_data[ch].sel(bias_mV = slicing_bias_mV, method=\"nearest\" ),\n",
    "                    ax =  axs[0],\n",
    "                    robust = True)\n",
    "    axs[0].hlines(sliderY.value,0,xr_data.X.shape[0], lw = 1, color = 'c')\n",
    "    axs[0].vlines(sliderX.value,0,xr_data.Y.shape[0], lw = 1, color = 'm')    \n",
    "\n",
    "    xr_data_Vline_profile.plot(ax = axs[1], robust = True)#, vmin = xr_data_Vline_profile.to_numpy().min() , vmax = xr_data_Vline_profile.to_numpy().max())\n",
    "    xr_data_Hline_profile.T.plot(ax = axs[2], robust = True)#, vmin = xr_data_Hline_profile.to_numpy().min() , vmax = xr_data_Hline_profile.to_numpy().max())\n",
    "\n",
    "    xr_data[ch].isel(X =sliderX.value, Y =sliderY.value) .plot(ax =axs[3])\n",
    "    #pn.Row(pn.Column(dmap_slideXY,xr_data_Vline_profile.plot()), )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb78bab-7054-4eb6-9746-dda832324c4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_Xslice_w_LDOS (xr_data, sliderX, ch ='LIX_fb', slicing_bias_mV = 0):\n",
    "    \n",
    "    '''\n",
    "    ################################\n",
    "    # use the slider in advance \n",
    "    sliderX = pnw.IntSlider(name='X', \n",
    "                           start = 0 ,\n",
    "                           end = grid_3D.X.shape[0]) \n",
    "    sliderY = pnw.IntSlider(name='Y', \n",
    "                           start = 0 ,\n",
    "                           end = grid_3D.Y.shape[0]) \n",
    "\n",
    "    #sliderX_v_intact = interact(lambda x:  grid_3D.X[x].values, x =sliderX)[1]\n",
    "    #sliderY_v_intact = interact(lambda y:  grid_3D.Y[y].values, y =sliderY)[1]\n",
    "    pn.Column(interact(lambda x:  grid_3D.X[x].values, x =sliderX), interact(lambda y: grid_3D.Y[y].values, y =sliderY))\n",
    "\n",
    "    ####################################\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    print(\"use the sliderX&Y first\")\n",
    "    #plt.style.use('default')\n",
    "    sliderX_v = xr_data.X[sliderX.value].values\n",
    "    #sliderY_v = xr_data.Y[sliderY.value].values\n",
    "\n",
    "\n",
    "    xr_data_Hline_profile = xr_data.isel(Y = sliderY.value)[ch]\n",
    "\n",
    "    xr_data_Vline_profile = xr_data.isel(X = sliderX.value)[ch]\n",
    "    \n",
    "    # bias_mV slicing\n",
    "    fig,axes = plt.subplots (nrows = 3,\n",
    "                            ncols = 1,\n",
    "                            figsize = (3,6))\n",
    "    axs = axes.ravel()\n",
    "\n",
    "    isns.imshow(xr_data[ch].sel(bias_mV = slicing_bias_mV, method=\"nearest\" ),\n",
    "                    ax =  axs[0],\n",
    "                    robust = True)\n",
    "    axs[0].hlines(sliderY.value,0,xr_data.X.shape[0], lw = 1, color = 'c')\n",
    "    axs[0].vlines(sliderX.value,0,xr_data.Y.shape[0], lw = 1, color = 'm')    \n",
    "\n",
    "    xr_data_Vline_profile.plot(ax = axs[1], robust = True)#, vmin = xr_data_Vline_profile.to_numpy().min() , vmax = xr_data_Vline_profile.to_numpy().max())\n",
    "    #xr_data_Hline_profile.T.plot(ax = axs[2], robust = True)#, vmin = xr_data_Hline_profile.to_numpy().min() , vmax = xr_data_Hline_profile.to_numpy().max())\n",
    "\n",
    "    xr_data[ch].isel(X =sliderX.value, Y =sliderY.value) .plot(ax =axs[2])\n",
    "    #pn.Row(pn.Column(dmap_slideXY,xr_data_Vline_profile.plot()), )\n",
    "\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4a137-e23f-45b2-92e4-fb32ff8f6135",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_0plateau_gap(xr_data,tolerance_I =  0.2E-11, tolerance_LIX = 1E-11, apply_SGfilter = True):\n",
    "    '''\n",
    "    check the tolerance_I  &  tolerance_LIx values in advance \n",
    "    USE 'find_plateau_tolarence_values' function . \n",
    "    \n",
    "    plateau finding only. \n",
    "    apply SG & 1st , 2nd derivative in advance. \n",
    "    Using both I_fb & LIX_fb, \n",
    "    assign plateau as an intersection of I_fb & LIX_fb plateau \n",
    "    \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    xr_data_prcssd = xr_data.copy(deep = True)\n",
    "    print('Find plateau in I &LIX each points')\n",
    "    if apply_SGfilter == True :\n",
    "        print('import savgolFilter_xr in advance' )\n",
    "        xr_data_sg = savgolFilter_xr(xr_data_prcssd, window_length = 21, polyorder = 3)\n",
    "\n",
    "    else : \n",
    "        print ('without SavgolFilter_xr, check outliers')\n",
    "        xr_data_sg = xr_data_prcssd\n",
    "\n",
    "    if 'I_fb' in xr_data.data_vars : \n",
    "        I_fb_plateau = abs(xr_data_sg['I_fb']) <= tolerance_I \n",
    "    else :\n",
    "        I_fb_plateau = abs(xr_data_sg['LIX_fb']) <= tolerance_LIx  \n",
    "        print ('No I_fb channel, use LIX instead')\n",
    "\n",
    "    if 'LIX_unit_calc' in xr_data.data_vars : \n",
    "        LIX_fb_plateau = abs(xr_data_sg['LIX_unit_calc']) <= tolerance_LIX \n",
    "    else: \n",
    "        LIX_fb_plateau = abs(xr_data_sg['LIX_fb']) <= tolerance_LIX \n",
    "        print ('No LIX_unit_calc channel, use LIX instead')\n",
    "\n",
    "    I_LIX_plateau = I_fb_plateau*LIX_fb_plateau\n",
    "    # pixels in X,Y, bias_mV  intersection of plateau\n",
    "  \n",
    "\n",
    "    xr_data_sg['I_LIX_plateau']=I_LIX_plateau\n",
    "    \n",
    "    # out figure\n",
    "    gap_pos0_I = xr_data.where(I_LIX_plateau).I_fb.idxmax(dim='bias_mV')\n",
    "    gap_neg0_I = xr_data.where(I_LIX_plateau).I_fb.idxmin(dim='bias_mV')\n",
    "    gap_mapI = gap_pos0_I-gap_neg0_I\n",
    "\n",
    "    gap_pos0_LIX = xr_data.where(I_LIX_plateau).LIX_unit_calc.idxmax(dim='bias_mV')\n",
    "    gap_neg0_LIX = xr_data.where(I_LIX_plateau).LIX_unit_calc.idxmin(dim='bias_mV')\n",
    "    gap_map_LIX = gap_pos0_LIX - gap_neg0_LIX\n",
    "\n",
    "    fig,axes = plt.subplots(ncols=3, nrows = 2 , figsize= (9,6))\n",
    "    axs= axes.ravel()\n",
    "    gap_pos0_I.plot(ax = axs[0] )\n",
    "    axs[0].set_title('gap_I_0+')\n",
    "    gap_neg0_I.plot(ax = axs[1])\n",
    "    axs[1].set_title('gap_I_0+')\n",
    "    gap_mapI.plot(ax = axs[2])\n",
    "    axs[2].set_title('gap_map_I')\n",
    "\n",
    "    gap_pos0_LIX.plot(ax = axs[3])\n",
    "    axs[3].set_title('gap_LIX_0+')\n",
    "    gap_neg0_LIX.plot(ax = axs[4])\n",
    "    axs[4].set_title('gap_LIX_0+')\n",
    "    gap_map_LIX.plot(ax = axs[5])\n",
    "    axs[5].set_title('gap_map_LIX')\n",
    "    axs[0].set_aspect(1)\n",
    "    axs[1].set_aspect(1)\n",
    "    axs[2].set_aspect(1)\n",
    "    axs[3].set_aspect(1)\n",
    "    axs[4].set_aspect(1)\n",
    "    axs[5].set_aspect(1)\n",
    "    fig.suptitle('tolerance_I = '+str(tolerance_I) + \"& tolerance_LIX = \" + str(tolerance_LIX) )\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    return xr_data_sg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b6faaa-50e3-4ff9-8d98-b4476d9dd80d",
   "metadata": {
    "lines_to_next_cell": 0,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def find_plateau_tolarence_values (xr_data, x_i ,  y_j ,ch ='LIX_fb',slicing_bias_mV = 2, tolerance_I= 1E-10, tolerance_LIX = 1E-12):\n",
    "    '''\n",
    "    Use slider in advance. \n",
    "    check XY position with \n",
    "    \"plot_XYslice_w_LDOS\" function \n",
    "    \n",
    "        #### use the slider \n",
    "        sliderX = pnw.IntSlider(name='X', \n",
    "                           start = 0 ,\n",
    "                           end = grid_3D.X.shape[0]) \n",
    "        sliderY = pnw.IntSlider(name='Y', \n",
    "                           start = 0 ,\n",
    "                           end = grid_3D.Y.shape[0]) \n",
    "\n",
    "        #sliderX_v_intact = interact(lambda x:  grid_3D.X[x].values, x =sliderX)[1]\n",
    "        #sliderY_v_intact = interact(lambda y:  grid_3D.Y[y].values, y =sliderY)[1]\n",
    "        pn.Column(interact(lambda x:  grid_3D.X[x].values, x =sliderX), interact(lambda y: grid_3D.Y[y].values, y =sliderY))\n",
    "        # Do not exceed the max Limit ==> error\n",
    "        # how to connect interactive values to the other cell --> need to update (later) \n",
    "        x_i = sliderX.value\n",
    "        y_j = sliderY.value \n",
    "    \n",
    "    \n",
    "    '''\n",
    "\n",
    "    print (x_i ,y_j)\n",
    "\n",
    "    fig,axes =  plt.subplots (ncols = 3, figsize = (9,3))\n",
    "    axs = axes.ravel()\n",
    "    \n",
    "    # plot 2D map with x_i & y_j \n",
    "    \n",
    "    isns.imshow(xr_data[ch].sel(bias_mV = slicing_bias_mV, method=\"nearest\" ),\n",
    "                    ax =  axs[0],\n",
    "                    robust = True)\n",
    "    axs[0].hlines(y_j,0,xr_data.X.shape[0], lw = 1, color = 'c')\n",
    "    axs[0].vlines(x_i,0,xr_data.Y.shape[0], lw = 1, color = 'm')  \n",
    "    \n",
    "    \n",
    "    \n",
    "    # for I_fb\n",
    "    sns.lineplot (xr_data.I_fb.isel(X = x_i, Y = y_j).to_dataframe(), x= 'bias_mV',y= 'I_fb', ax =axs[1])\n",
    "    axs[1].axhline(y=tolerance_I, c='orange') # pos tolerance line\n",
    "    axs[1].axhline(y=-tolerance_I, c='orange') # neg tolerance line\n",
    "    # fill between x area where Y value is smaller than tolerance value \n",
    "    axs[1].fill_between(xr_data.I_fb.isel(X = x_i, Y = y_j).bias_mV, -tolerance_I, tolerance_I, \n",
    "                   where=abs(xr_data.I_fb.isel(X = x_i, Y = y_j)) <= tolerance_I,\n",
    "                   facecolor='yellow', interpolate=True, alpha=0.3)\n",
    "    \n",
    "    # for LIX_fb\n",
    "    sns.lineplot (xr_data.LIX_fb.isel(X = x_i, Y = y_j).to_dataframe(), x= 'bias_mV',y= 'LIX_fb', ax =axs[2])\n",
    "    axs[2].axhline(y=tolerance_LIX, c='magenta') # pos tolerance line\n",
    "    axs[2].axhline(y=-tolerance_LIX, c='magenta') # neg tolerance line\n",
    "    # fill between x area where Y value is smaller than tolerance value \n",
    "    axs[2].fill_between(xr_data.LIX_fb.isel(X = x_i, Y = y_j).bias_mV, -tolerance_LIX, tolerance_LIX, \n",
    "                   where=abs(xr_data.LIX_fb.isel(X = x_i, Y = y_j)) <= tolerance_LIX,\n",
    "                   facecolor='cyan', interpolate=True, alpha=0.3)\n",
    "                  \n",
    "    axs[2].fill_between(xr_data.I_fb.isel(X = x_i, Y = y_j).bias_mV, -tolerance_LIX, tolerance_LIX, \n",
    "                        where=abs(xr_data.I_fb.isel(X = x_i, Y = y_j)) <= tolerance_I,\n",
    "                        facecolor='yellow', interpolate=True, alpha=0.3)\n",
    "                  \n",
    "                  \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d8c15e-b915-4756-a589-33c2b615761e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peaks_xr(xrdata, height= None, threshold=None, distance=None, prominence = None, width=None): \n",
    "    from scipy.signal import find_peaks\n",
    "    xrdata_prcssd = xrdata.copy(deep = True)\n",
    "    print('Find peaks in STS to an xarray Dataset.')\n",
    "\n",
    "    for data_ch in xrdata:\n",
    "        if len(xrdata[data_ch].dims)==2:\n",
    "            # smoothing filter only for the 3D data set\n",
    "                    # ==> updated             \n",
    "            \n",
    "            \n",
    "\n",
    "            ### 2D data case \n",
    "            ### assume that coords are 'X','Y','bias_mV'\n",
    "            #### two case X,bias_mV or Y,bias_mV \n",
    "            if 'X' in xrdata[data_ch].dims :\n",
    "                # xrdata is X,bias_mV \n",
    "                # use the isel(X = x) \n",
    "                x_axis = xrdata.X.size\n",
    "\n",
    "                #print(xrdata_prcssd[data_ch])\n",
    "\n",
    "                xrdata_prcssd[data_ch+'_peaks'] = xr.DataArray (\n",
    "                    np.array([ find_peaks(xrdata[data_ch].isel(X = x).values, height =height, distance = distance, threshold = threshold, prominence= prominence, width =width)\n",
    "                              for x in range(x_axis)], dtype = object )[:,0],\n",
    "                dims=[\"X\"],\n",
    "                coords={\"X\": xrdata.X})\n",
    "            \n",
    "            elif 'Y' in xrdata[data_ch].dims :\n",
    "                # xrdata is Y,bias_mV \n",
    "                # use the isel(Y = y) \n",
    "                y_axis = xrdata.Y.size\n",
    "\n",
    "                #print(xrdata_prcssd[data_ch])\n",
    "\n",
    "                xrdata_prcssd[data_ch+'_peaks'] = xr.DataArray (\n",
    "                    np.array([ find_peaks(xrdata[data_ch].isel(Y = y).values, height =height, distance = distance,threshold = threshold, prominence=prominence, width=width)\n",
    "                              for y in range(y_axis)], dtype = object )[:,0],\n",
    "                dims=[\"Y\"],\n",
    "                coords={\"Y\": xrdata.Y})\n",
    "            \n",
    "            # ==> updated \n",
    "            \n",
    "        elif len(xrdata[data_ch].dims) == 3:\n",
    "            \n",
    "            x_axis = xrdata.X.size\n",
    "            y_axis = xrdata.Y.size\n",
    "            print (data_ch)\n",
    "            xrdata_prcssd[data_ch+'_peaks'] = xr.DataArray (\n",
    "                np.array([ find_peaks(xrdata[data_ch].isel(X = x, Y = y).values, height =height, distance = distance,threshold = threshold,prominence=prominence, width=width)[0] \n",
    "                          for y in range(y_axis)  \n",
    "                          for x in range(x_axis)], dtype = object ).reshape(x_axis,y_axis),\n",
    "                dims=[\"X\", \"Y\"],\n",
    "                coords={\"X\": xrdata.X, \"Y\": xrdata.Y})         \n",
    "        elif len(xrdata[data_ch].dims) == 1:\n",
    "            if 'bias_mV' in xrdata.dims: \n",
    "                for data_ch in xrdata: \n",
    "                    xrdata_prcssd[data_ch+'_peaks'] = xr.DataArray (find_peaks (xrdata[data_ch], height =height, distance = distance,threshold = threshold, prominence= prominence, width= width))\n",
    "        else : pass\n",
    "    return xrdata_prcssd\n",
    "#grid_2D_sg_pks = find_peaks_xr(grid_2D_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ae9f64-51c8-4ee4-8838-c380546642c4",
   "metadata": {
    "lines_to_next_cell": 0
   },
   "outputs": [],
   "source": [
    "def peak_pad(xrdata, padding_value = np.nan):\n",
    "    xrdata_prcssd = xrdata.copy(deep = True)\n",
    "    \n",
    "    \n",
    "    \n",
    "    if len(xrdata.dims)==2:\n",
    "        for data_ch in xrdata:\n",
    "        \n",
    "               \n",
    "            # smoothing filter only for the 3D data set\n",
    "            ### 2D data case \n",
    "            ### assume that coords are 'X','Y','bias_mV'\n",
    "            #### two case X,bias_mV or Y,bias_mV \n",
    "            if 'X' in xrdata[data_ch].dims :\n",
    "                # xrdata is X,bias_mV \n",
    "                # use the isel(X = x) \n",
    "                x_axis = xrdata.X.size\n",
    "\n",
    "                if data_ch.endswith('_peaks'):\n",
    "                    peaks = xrdata[data_ch].values\n",
    "                    peaks_count_max = max([ len(peaks_r) \n",
    "                                    for peaks_r in peaks])\n",
    "                    #padding_value = np.nan\n",
    "                    #print(xrdata_prcssd[data_ch])\n",
    "                    \n",
    "                    peaks_pad = np.array(\n",
    "                        [ np.pad(peaks_r.astype(float), \n",
    "                                 (0,peaks_count_max-len(peaks_r)), mode = 'constant', \n",
    "                               constant_values = padding_value)\n",
    "                        for peaks_r in peaks ]\n",
    "                    ).reshape((x_axis,-1))\n",
    "                    \n",
    "                    xrdata_prcssd[data_ch+'_pad'] = xr.DataArray(peaks_pad, dims=[\"X\", \"peaks\"],\n",
    "                    coords={\"X\": xrdata.X, \"peaks\": np.arange(peaks_count_max)}).astype('int')\n",
    "                else: pass###\n",
    " \n",
    "            elif 'Y' in xrdata[data_ch].dims :\n",
    "                # xrdata is Y,bias_mV \n",
    "                # use the isel(Y = y) \n",
    "                y_axis = xrdata.Y.size\n",
    "                if data_ch.endswith('_peaks'):\n",
    "                    peaks = xrdata[data_ch].values\n",
    "                    peaks_count_max = max([ len(peaks_r) \n",
    "                                    for peaks_r in peaks])\n",
    "                    #padding_value = np.nan\n",
    "                    #print(xrdata_prcssd[data_ch])\n",
    "                    \n",
    "                    peaks_pad = np.array(\n",
    "                        [ np.pad(peaks_r.astype(float), \n",
    "                                 (0,peaks_count_max-len(peaks_r)), mode = 'constant', \n",
    "                               constant_values = padding_value)\n",
    "                        for peaks_r in peaks ]\n",
    "                    ).reshape((y_axis,-1))\n",
    "                    \n",
    "                    xrdata_prcssd[data_ch+'_pad'] = xr.DataArray(peaks_pad, dims=[\"Y\", \"peaks\"],\n",
    "                    coords={\"Y\": xrdata.Y, \"peaks\": np.arange(peaks_count_max)}).astype('int')\n",
    "                    #print(xrdata_prcssd[data_ch])\n",
    "                else: pass #\n",
    "            else: pass##\n",
    "\n",
    "    \n",
    "        #if len(xrdata[data_ch].dims)==2:\n",
    "            # smoothing filter only for the 3D data set\n",
    "            \n",
    "    elif len(xrdata.dims)==3:\n",
    "        x_axis = xrdata.X.size\n",
    "        y_axis = xrdata.Y.size\n",
    "        for data_ch in xrdata_prcssd:\n",
    "            if data_ch.endswith('_peaks'):\n",
    "                peaks = xrdata[data_ch].values\n",
    "                peaks_count_max = max([ len(peaks_r_c) \n",
    "                                for peaks_r in peaks\n",
    "                                for  peaks_r_c in peaks_r])\n",
    "                #padding_value = np.nan\n",
    "\n",
    "                peaks_pad = np.array([\n",
    "                    np.pad(peaks_r_c.astype(float), \n",
    "                           (0,peaks_count_max-len(peaks_r_c)),\n",
    "                           mode = 'constant', \n",
    "                           constant_values = padding_value)\n",
    "                    for peaks_r in peaks \n",
    "                    for  peaks_r_c in peaks_r]).reshape((x_axis,y_axis,-1))\n",
    "\n",
    "                xrdata_prcssd[data_ch+'_pad'] = xr.DataArray(peaks_pad, dims=[\"X\", \"Y\",\"peaks\"],\n",
    "                    coords={\"X\": xrdata.X, \"Y\": xrdata.Y, \"peaks\": np.arange(peaks_count_max)}).astype('int')\n",
    "            else: pass\n",
    "        \n",
    "        \n",
    "    elif len(xrdata.dims) == 1:\n",
    "        peaks_count_max_ch = []\n",
    "        for data_ch in xrdata:\n",
    "            if data_ch.endswith('_peaks'):\n",
    "                peaks_count_max_ch.append(len(xrdata[data_ch].to_numpy().tolist()[0]))\n",
    "        peaks_count_max = max(peaks_count_max_ch)\n",
    "        #print(peaks_count_max        )\n",
    "        for data_ch in xrdata:\n",
    "            if data_ch.endswith('_peaks'):\n",
    "                if len(xrdata[data_ch].to_numpy().tolist()[0]) != 0:\n",
    "                    #print(data_ch)\n",
    "                    peaks = np.array(xrdata[data_ch].to_numpy().tolist()[0])\n",
    "                    #print(peaks)\n",
    "                    #padding_value = np.nan\n",
    "                    #print((0,peaks_count_max-len(peaks)))\n",
    "                    #print(xrdata_prcssd[data_ch])\n",
    "                    \n",
    "                    peaks_pad =np.pad(peaks.astype('float32'),\n",
    "                                      (0,int(peaks_count_max-len(peaks))),\n",
    "                                      mode = 'constant', \n",
    "                                      constant_values = padding_value)\n",
    "                    \n",
    "                    #print(peaks_pad)\n",
    "                    xrdata_prcssd[data_ch+'_pad'] = xr.DataArray(peaks_pad, dims=[\"peaks\"],\n",
    "                            coords={ \"peaks\": np.arange(peaks_count_max)}).astype('int')\n",
    "                else: pass\n",
    "            else: pass\n",
    "    \n",
    "    return xrdata_prcssd\n",
    "        \n",
    "#grid_3D_sg_pks_pad = peak_pad(grid_3D_sg_pks)\n",
    "#grid_3D_sg_pks_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a527d184-b265-4303-8350-063a6d58feb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peaks_prominence_xr(xrdata, find_peaks_in_ch = 'LDOS_fb', height= None, threshold=None, distance=None): \n",
    "    from scipy.signal import find_peaks, peak_prominences\n",
    "    \n",
    "    xrdata_prcssd = xrdata.copy(deep = True)\n",
    "    \n",
    "    print('Use this function only after find_peaks_xr  & peak_pad')\n",
    "    # counting irregular number of dimension issue \n",
    "    # each pixel will have different pixel number \n",
    "    # use peak_pad for peak # as a dimension \n",
    "    print (' use padding_value= 0, & remove peaks at index zero' ) \n",
    "    # peak_pad filling --> 0 \n",
    "    \n",
    "    \n",
    "    for ch_i, data_ch in enumerate(xrdata):\n",
    "\n",
    "        if data_ch == find_peaks_in_ch:\n",
    "            print (data_ch + 'dims = '+ str(len(xrdata[data_ch].dims)))\n",
    "            # channel dim is not good variable to assign grid_line or grid_map\n",
    "            \n",
    "            if len(xrdata[data_ch].dims) == 1:\n",
    "                if data_ch == find_peaks_in_ch : \n",
    "                    print (data_ch+ ' peak_properties check for dim ==1')\n",
    "                    if 'bias_mV' in xrdata.dims: \n",
    "                        for data_ch in xrdata: \n",
    "                            xrdata_prcssd[data_ch+'_peaks_pad'] = xr.DataArray (peak_prominences(xrdata[data_ch].values[0,:], xrdata[data_ch+'_peaks_pad'].values[0,:])[0])\n",
    "                    else : pass\n",
    "                else: pass\n",
    "\n",
    "    \n",
    "            elif ( len(xrdata.X) == 1 ) or (len(xrdata.Y) == 1 ) :\n",
    "                print (data_ch+ ' peak_properties check for dim ==2')\n",
    "                # smoothing filter only for the 3D data set# ==> updated             \n",
    "                ### 2D data case \n",
    "                ### assume that coords are 'X','Y','bias_mV'\n",
    "                #### two case X,bias_mV or Y,bias_mV \n",
    "                if 'X' in xrdata[data_ch].dims :\n",
    "                    # xrdata is X,bias_mV \n",
    "                    # use the isel(X = x) \n",
    "                    x_axis = xrdata.X.size\n",
    "                    print('Along X')\n",
    "                    #print(xrdata_prcssd[data_ch])\n",
    "\n",
    "                    xrdata_prcssd[data_ch+'_peak_prominence'] = xr.DataArray (\n",
    "                        np.array([ peak_prominences(xrdata[data_ch].isel(X = x).values[0,:], xrdata[data_ch+'_peaks_pad'].isel(X = x).values[0,:])\n",
    "                                  for x in range(x_axis)], dtype = float ),\n",
    "                    dims=[\"X\", \"prominence\", \"peaks\"],\n",
    "                    coords={\"X\": xrdata.X, \"peaks\": xrdata.peaks, \"prominence\":['prominences', 'left_bases','right_basis']})\n",
    "\n",
    "                elif 'Y' in xrdata[data_ch].dims :\n",
    "                    # xrdata is Y,bias_mV \n",
    "                    # use the isel(Y = y) \n",
    "                    y_axis = xrdata.Y.size\n",
    "                    print('Along Y')\n",
    "                    #print(xrdata_prcssd[data_ch])\n",
    "\n",
    "                    xrdata_prcssd[data_ch+'_peaks'] = xr.DataArray (\n",
    "                        np.array([peak_prominences(xrdata[data_ch].isel(Y = y).values[0,:], xrdata[data_ch+'_peaks_pad'].isel(Y = y).values[0,:])\n",
    "                                  for y in range(y_axis)], dtype = float ),\n",
    "                    dims=[\"Y\", \"prominence\", \"peaks\"],\n",
    "                    coords={\"Y\": xrdata.Y, \"peaks\": xrdata.peaks, \"prominence\":['prominences', 'left_bases','right_basis']})\n",
    "                else: \n",
    "                     print (data_ch + ': channel is not for prominence finding dim==2')\n",
    "                    # ==> updated \n",
    "\n",
    "            elif ( len(xrdata.X) != 1 ) & (len(xrdata.Y) != 1 ) :\n",
    "                if data_ch == find_peaks_in_ch : \n",
    "\n",
    "                    print('dim ==3')\n",
    "                    x_axis = xrdata.X.size\n",
    "                    y_axis = xrdata.Y.size\n",
    "                    print (ch_i,data_ch)\n",
    "                    print ('prominence checking')\n",
    "                    xrdata_prcssd[data_ch+'_peaks_prominience'] = xr.DataArray (\n",
    "                        np.array([ peak_prominences(xrdata[data_ch].isel(X = x, Y = y).values[0,:], xrdata[data_ch+'_peaks_pad'].isel(X = x, Y = y).values[0,:])[0]\n",
    "                                  for y in range(y_axis)  \n",
    "                                  for x in range(x_axis)], dtype = float ).reshape(x_axis,y_axis),\n",
    "                        dims=[\"X\", \"Y\",\"peaks\",\"prominence\" ],\n",
    "                        coords={\"X\": xrdata.X, \"Y\": xrdata.Y, \"peaks\": xrdata.peaks, \"prominence\":['prominences', 'left_bases','right_basis']})\n",
    "\n",
    "                    ### there is something wrong here...\n",
    "                    ###  check the find peak functions again ..\n",
    "                else:                     \n",
    "                    print (data_ch + str(ch_i)+ ': channel is not for prominence finding, dim ==3')\n",
    "                    print('_peak_prominence_skip')\n",
    "                    #xrdata_prcssd[data_ch] = xrdata[data_ch]\n",
    "                    print (data_ch, ch_i)\n",
    "                    print (data_ch+ ' peak_properties check not for this c hannel , for dim ==3')\n",
    "            else: pass\n",
    "    \n",
    "                                            \n",
    "        else : pass\n",
    "        \n",
    "    return xrdata_prcssd\n",
    "#grid_2D_sg_pks = find_peaks_xr(grid_2D_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a0df898-064b-497d-8bfa-9163ebe792f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_peaks_properties_xr(xrdata, find_peaks_in_ch = 'LDOS_fb', height= None, threshold=None, distance=None): \n",
    "    from scipy.signal import find_peaks, peak_prominences\n",
    "    \n",
    "    xrdata_prcssd = xrdata.copy(deep = True)\n",
    "    \n",
    "    print('Use this function only after find_peaks_xr  & peak_pad')\n",
    "    # counting irregular number of dimension issue \n",
    "    # each pixel will have different pixel number \n",
    "    # use peak_pad for peak # as a dimension \n",
    "    print (' use padding_value= 0, & remove peaks at index zero' ) \n",
    "    print (' this function will be updated later for properties dict in case of prominence or width was given for peak finding ' )\n",
    "    # peak_pad filling --> 0 \n",
    "    \n",
    "    \n",
    "    for ch_i, data_ch in enumerate(xrdata):\n",
    "\n",
    "        if data_ch == find_peaks_in_ch:\n",
    "            print (data_ch + 'dims = '+ str(len(xrdata[data_ch].dims)))\n",
    "            # channel dim is not good variable to assign grid_line or grid_map\n",
    "            \n",
    "            if len(xrdata[data_ch].dims) == 1:\n",
    "                if data_ch == find_peaks_in_ch : \n",
    "                    print (data_ch+ ' peak_properties check for dim ==1')\n",
    "                    if 'bias_mV' in xrdata.dims: \n",
    "                        for data_ch in xrdata: \n",
    "                            xrdata_prcssd[data_ch+'_peaks_pad'] = xr.DataArray (peak_prominences(xrdata[data_ch].values[0,:], xrdata[data_ch+'_peaks_pad'].values[0,:])[0])\n",
    "                    else : pass\n",
    "                else: pass\n",
    "\n",
    "    \n",
    "            elif ( len(grid_LDOS_sg_pk.X) == 1 ) or (len(grid_LDOS_sg_pk.Y) == 1 ) :\n",
    "                print (data_ch+ ' peak_properties check for dim ==2')\n",
    "                # smoothing filter only for the 3D data set# ==> updated             \n",
    "                ### 2D data case \n",
    "                ### assume that coords are 'X','Y','bias_mV'\n",
    "                #### two case X,bias_mV or Y,bias_mV \n",
    "                if 'X' in xrdata[data_ch].dims :\n",
    "                    # xrdata is X,bias_mV \n",
    "                    # use the isel(X = x) \n",
    "                    x_axis = xrdata.X.size\n",
    "                    print('Along X')\n",
    "                    #print(xrdata_prcssd[data_ch])\n",
    "\n",
    "                    xrdata_prcssd[data_ch+'_peak_prominence'] = xr.DataArray (\n",
    "                        np.array([ peak_prominences(xrdata[data_ch].isel(X = x).values[0,:], xrdata[data_ch+'_peaks_pad'].isel(X = x).values[0,:])\n",
    "                                  for x in range(x_axis)], dtype = float ),\n",
    "                    dims=[\"X\", \"prominence\", \"peaks\"],\n",
    "                    coords={\"X\": xrdata.X, \"peaks\": xrdata.peaks, \"prominence\":['prominences', 'left_bases','right_basis']})\n",
    "\n",
    "                elif 'Y' in xrdata[data_ch].dims :\n",
    "                    # xrdata is Y,bias_mV \n",
    "                    # use the isel(Y = y) \n",
    "                    y_axis = xrdata.Y.size\n",
    "                    print('Along Y')\n",
    "                    #print(xrdata_prcssd[data_ch])\n",
    "\n",
    "                    xrdata_prcssd[data_ch+'_peaks'] = xr.DataArray (\n",
    "                        np.array([peak_prominences(xrdata[data_ch].isel(Y = y).values[0,:], xrdata[data_ch+'_peaks_pad'].isel(Y = y).values[0,:])\n",
    "                                  for y in range(y_axis)], dtype = float ),\n",
    "                    dims=[\"Y\", \"prominence\", \"peaks\"],\n",
    "                    coords={\"Y\": xrdata.Y, \"peaks\": xrdata.peaks, \"prominence\":['prominences', 'left_bases','right_basis']})\n",
    "                else: \n",
    "                     print (data_ch + ': channel is not for prominence finding dim==2')\n",
    "                    # ==> updated \n",
    "\n",
    "            elif ( len(grid_LDOS_sg_pk.X) != 1 ) & (len(grid_LDOS_sg_pk.Y) != 1 ) :\n",
    "                if data_ch == find_peaks_in_ch : \n",
    "\n",
    "                    print('dim ==3')\n",
    "                    x_axis = xrdata.X.size\n",
    "                    y_axis = xrdata.Y.size\n",
    "                    print (ch_i,data_ch)\n",
    "                    print ('prominence checking')\n",
    "                    xrdata_prcssd[data_ch+'_peaks_prominience'] = xr.DataArray (\n",
    "                        np.array([ peak_prominences(xrdata[data_ch].isel(X = x, Y = y).values[0,:], xrdata[data_ch+'_peaks_pad'].isel(X = x, Y = y).values[0,:])[0]\n",
    "                                  for y in range(y_axis)  \n",
    "                                  for x in range(x_axis)], dtype = float ).reshape(x_axis,y_axis),\n",
    "                        dims=[\"X\", \"Y\",\"peaks\",\"prominence\" ],\n",
    "                        coords={\"X\": xrdata.X, \"Y\": xrdata.Y, \"peaks\": xrdata.peaks, \"prominence\":['prominences', 'left_bases','right_basis']})\n",
    "\n",
    "                    ### there is something wrong here...\n",
    "                    ###  check the find peak functions again ..\n",
    "                else:                     \n",
    "                    print (data_ch + str(ch_i)+ ': channel is not for prominence finding, dim ==3')\n",
    "                    print('_peak_prominence_skip')\n",
    "                    #xrdata_prcssd[data_ch] = xrdata[data_ch]\n",
    "                    print (data_ch, ch_i)\n",
    "                    print (data_ch+ ' peak_properties check not for this c hannel , for dim ==3')\n",
    "            else: pass\n",
    "    \n",
    "                                            \n",
    "        else : pass\n",
    "        \n",
    "    return xrdata_prcssd\n",
    "#grid_2D_sg_pks = find_peaks_xr(grid_2D_sg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7807c5d7-0c10-456b-9a95-17d2c587f77c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def peak_mV_3Dxr(xr_data,ch='LIX_fb'): \n",
    "    '''\n",
    "    after peak finding, \n",
    "    _peaks channels --> \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    #after find_peaks_xr \n",
    "    xrdata_prcssd = xr_data.copy(deep = True)\n",
    "    print('After peak finding in STS, marking in the 3D data')\n",
    "    x_axis = xr_data.X.size\n",
    "    y_axis = xr_data.Y.size\n",
    "    bias_mV_axis = xr_data.bias_mV.size\n",
    "    \n",
    "    peaks_list = xr_data[ch+'_peaks'].values\n",
    "    for data_ch in xr_data:\n",
    "        if '_peaks' in data_ch:\n",
    "            pass\n",
    "        # do nothing for channels with_peaks information  \n",
    "        else: \n",
    "            xrdata_prcssd[data_ch+'_peaks_mV'] = xr.DataArray (\n",
    "                np.array([ xr_data.bias_mV.isin(xr_data.bias_mV[peaks_list[x,y]])\n",
    "                          for y in range(y_axis)  \n",
    "                          for x in range(x_axis)], dtype = object ).reshape(y_axis,x_axis,bias_mV_axis),\n",
    "                dims=[\"Y\", \"X\",\"bias_mV\"],\n",
    "                coords={\"X\": xr_data.X, \"Y\": xr_data.Y,  \"bias_mV\": xr_data.bias_mV}) \n",
    "    return xrdata_prcssd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdf0ed68-e77c-457f-8af3-cf1306425a16",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def peak_pad(xrdata):\n",
    "    xrdata_prcssd = xrdata.copy(deep = True)\n",
    "    xAxis = xrdata.X.size\n",
    "    yAxis = xrdata.Y.size\n",
    "    for ch in xrdata_prcssd:\n",
    "        if ch.endswith('_peaks'):\n",
    "            peaks = xrdata[ch].values\n",
    "            peaks_count_max = max([ len(peaks_r_c) \n",
    "                            for peaks_r in peaks\n",
    "                            for  peaks_r_c in peaks_r])\n",
    "            padding_value = np.nan\n",
    "            \n",
    "            peaks_pad = np.array([\n",
    "                np.pad(peaks_r_c.astype(float), \n",
    "                       (0,peaks_count_max-len(peaks_r_c)),\n",
    "                       mode = 'constant', \n",
    "                       constant_values = padding_value)\n",
    "                for peaks_r in peaks \n",
    "                for  peaks_r_c in peaks_r]).reshape((xAxis,yAxis,-1))\n",
    "            \n",
    "            xrdata_prcssd[ch+'_pad'] = xr.DataArray(peaks_pad, dims=[\"X\", \"Y\",\"peaks\"],\n",
    "                coords={\"X\": xrdata.X, \"Y\": xrdata.Y, \"peaks\": np.arange(peaks_count_max)})\n",
    "        else: pass\n",
    "    return xrdata_prcssd\n",
    "        \n",
    "#grid_3D_sg_pks_pad = peak_pad(grid_3D_sg_pks)\n",
    "#grid_3D_sg_pks_pad\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a346c44-dd11-47b1-aeb4-13a11b512294",
   "metadata": {},
   "source": [
    "### Rotating the 3D data "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf5c87aa-40d6-4026-a11f-059ca7760913",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3D rotation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5e099e-5a39-438e-8b53-938383c99a6a",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Xr rotation function \n",
    "# rotate the XY plan in xr data \n",
    "def rotate_3D_xr (xrdata, rotation_angle): \n",
    "    # padding first \n",
    "    for ch_i,ch_name in enumerate (xrdata):\n",
    "        if ch_i == 0:  # use only the first channel to calculate a padding size \n",
    "            padding_shape = skimage.transform.rotate(xrdata[ch_name].values.astype('float64'),\n",
    "                                                     rotation_angle,\n",
    "                                                     resize = True).shape[:2]\n",
    "            # After rotation, still 3D shape ->  [:2]\n",
    "            \n",
    "            padding_xy = (np.array( padding_shape)-np.array(xrdata[ch_name].shape[:2]) +1)/2\n",
    "            padding_xy = padding_xy.astype(int)\n",
    "    xrdata_pad = xrdata.pad(X=(padding_xy[0],padding_xy[0]), \n",
    "                            Y =(padding_xy[1],padding_xy[1]),\n",
    "                            mode='constant',\n",
    "                            cval = xrdata.min())\n",
    "    if np.array(xrdata_pad[ch_name]).shape[:2] != padding_shape:\n",
    "        # in case of xrdata_pad shape is +1 larger than real padding_shape\n",
    "        # index 다루는 법  (X)\n",
    "        x_spacing = np.diff(xrdata.X).mean()\n",
    "        y_spacing = np.diff(xrdata.Y).mean()\n",
    "        xrdata.X[0]\n",
    "        xrdata.Y[0]\n",
    "\n",
    "        x_pad_dim = padding_shape[0]#int(padding_xy[0]*2+xrdata.X.shape[0])\n",
    "        y_pad_dim = padding_shape[1]#int(padding_xy[0]*2+xrdata.Y.shape[0])\n",
    "\n",
    "        x_pad_arr =  np.linspace(-1*padding_xy[0]*x_spacing, x_spacing*x_pad_dim,x_pad_dim+1)\n",
    "        y_pad_arr =  np.linspace(-1*padding_xy[1]*y_spacing, y_spacing*y_pad_dim,y_pad_dim+1)\n",
    "\n",
    "        # 0 에서 전체 크기 만큼 padding 한결과를 array 만들고 offset 은 pad_x 만큼 \n",
    "        x_pad_arr.shape\n",
    "        y_pad_arr.shape\n",
    "        xrdata_pad = xrdata_pad.assign_coords( {\"X\" :  x_pad_arr}).assign_coords({\"Y\" :  y_pad_arr})\n",
    "        xrdata_rot = xrdata_pad.sel(X = xrdata_pad.X[:-1].values, Y = xrdata_pad.Y[:-1].values)\n",
    "        print ('padding size != rot_size')\n",
    "    else : # np.array(xrdata_pad[ch_name]).shape == padding_shape \n",
    "            # in case of xrdata_pad shape is +1 larger than real padding_shape\n",
    "\n",
    "        # index 다루는 법  (X)\n",
    "        x_spacing = np.diff(xrdata.X).mean()\n",
    "        y_spacing = np.diff(xrdata.Y).mean()\n",
    "        xrdata.X[0]\n",
    "        xrdata.Y[0]\n",
    "\n",
    "        x_pad_dim = padding_shape[0]#int(padding_xy[0]*2+xrdata.X.shape[0])\n",
    "        y_pad_dim = padding_shape[1]#int(padding_xy[0]*2+xrdata.Y.shape[0])\n",
    "\n",
    "        x_pad_arr =  np.linspace(-1*padding_xy[0]*x_spacing, x_spacing*x_pad_dim,x_pad_dim)\n",
    "        y_pad_arr =  np.linspace(-1*padding_xy[1]*y_spacing, y_spacing*y_pad_dim,y_pad_dim)\n",
    "\n",
    "        # 0 에서 전체 크기 만큼 padding 한결과를 array 만들고 offset 은 pad_x 만큼 \n",
    "        x_pad_arr.shape\n",
    "        y_pad_arr.shape\n",
    "        xrdata_pad = xrdata_pad.assign_coords( {\"X\" :  x_pad_arr}).assign_coords({\"Y\" :  y_pad_arr})\n",
    "        xrdata_rot = xrdata_pad.copy()      \n",
    "        print ('padding size == rot_size')\n",
    "    # call 1 channel\n",
    "        # use the list_comprehension for bias_mV range\n",
    "        # list comprehension is more faster\n",
    "        # after rotation, resize = False! , or replacement size error! \n",
    "        # replace the channel(padded) 3D data as a new 3D (rotated )data set \n",
    "\n",
    "    for ch in xrdata_pad:\n",
    "        xrdata_rot[ch].values = skimage.transform.rotate(xrdata[ch].values.astype('float64'),\n",
    "                                                         rotation_angle,\n",
    "                                                         cval =xrdata[ch].to_numpy().min(),\n",
    "                                                         resize = True)\n",
    "    return xrdata_rot\n",
    "# ### average X or Y direction jof Grid_3D dataset \n",
    "# * use xr_data (3D)\n",
    "# * average_in = 'X' or 'Y'\n",
    "# * ch_l_name = channel name for line profile  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3477bb-635f-45ac-94f0-2a500db7a350",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Xr rotation function \n",
    "# rotate the XY plan in xr data \n",
    "def rotate_3D_fft_xr (xrdata, rotation_angle): \n",
    "    # padding first \n",
    "    for ch_i,ch_name in enumerate (xrdata):\n",
    "        if ch_i == 0:  # use only the first channel to calculate a padding size \n",
    "            padding_shape = skimage.transform.rotate(xrdata[ch_name].values.astype('float64'),\n",
    "                                                     rotation_angle,\n",
    "                                                     resize = True).shape[:2]\n",
    "            # After rotation, still 3D shape ->  [:2]\n",
    "            \n",
    "            padding_xy = (np.array( padding_shape)-np.array(xrdata[ch_name].shape[:2]) +1)/2\n",
    "            padding_xy = padding_xy.astype(int)\n",
    "    xrdata_pad = xrdata.pad(freq_X=(padding_xy[0],padding_xy[0]), \n",
    "                            freq_Y =(padding_xy[1],padding_xy[1]),\n",
    "                            mode='constant',\n",
    "                            cval = xrdata.min())\n",
    "    if np.array(xrdata_pad[ch_name]).shape[:2] != padding_shape:\n",
    "        # in case of xrdata_pad shape is +1 larger than real padding_shape\n",
    "        # index 다루는 법  (X)\n",
    "        x_spacing = np.diff(xrdata.freq_X).mean()\n",
    "        y_spacing = np.diff(xrdata.freq_Y).mean()\n",
    "        xrdata.freq_X[0]\n",
    "        xrdata.freq_Y[0]\n",
    "\n",
    "        x_pad_dim = padding_shape[0]#int(padding_xy[0]*2+xrdata.X.shape[0])\n",
    "        y_pad_dim = padding_shape[1]#int(padding_xy[0]*2+xrdata.Y.shape[0])\n",
    "\n",
    "        x_pad_arr =  np.linspace(-1*padding_xy[0]*x_spacing, x_spacing*x_pad_dim,x_pad_dim+1)\n",
    "        y_pad_arr =  np.linspace(-1*padding_xy[1]*y_spacing, y_spacing*y_pad_dim,y_pad_dim+1)\n",
    "\n",
    "        # 0 에서 전체 크기 만큼 padding 한결과를 array 만들고 offset 은 pad_x 만큼 \n",
    "        x_pad_arr.shape\n",
    "        y_pad_arr.shape\n",
    "        xrdata_pad = xrdata_pad.assign_coords( {\"freq_X\" :  x_pad_arr}).assign_coords({\"freq_Y\" :  y_pad_arr})\n",
    "        xrdata_rot = xrdata_pad.sel(freq_X = xrdata_pad.freq_X[:-1].values, freq_Y = xrdata_pad.freq_Y[:-1].values)\n",
    "        print ('padding size != rot_size')\n",
    "    else : # np.array(xrdata_pad[ch_name]).shape == padding_shape \n",
    "            # in case of xrdata_pad shape is +1 larger than real padding_shape\n",
    "\n",
    "        # index 다루는 법  (X)\n",
    "        x_spacing = np.diff(xrdata.freq_X).mean()\n",
    "        y_spacing = np.diff(xrdata.freq_Y).mean()\n",
    "        xrdata.freq_X[0]\n",
    "        xrdata.freq_Y[0]\n",
    "\n",
    "        x_pad_dim = padding_shape[0]#int(padding_xy[0]*2+xrdata.X.shape[0])\n",
    "        y_pad_dim = padding_shape[1]#int(padding_xy[0]*2+xrdata.Y.shape[0])\n",
    "\n",
    "        x_pad_arr =  np.linspace(-1*padding_xy[0]*x_spacing, x_spacing*x_pad_dim,x_pad_dim)\n",
    "        y_pad_arr =  np.linspace(-1*padding_xy[1]*y_spacing, y_spacing*y_pad_dim,y_pad_dim)\n",
    "\n",
    "        # 0 에서 전체 크기 만큼 padding 한결과를 array 만들고 offset 은 pad_x 만큼 \n",
    "        x_pad_arr.shape\n",
    "        y_pad_arr.shape\n",
    "        xrdata_pad = xrdata_pad.assign_coords( {\"freq_X\" :  x_pad_arr}).assign_coords({\"freq_Y\" :  y_pad_arr})\n",
    "        xrdata_rot = xrdata_pad.copy()      \n",
    "        print ('padding size == rot_size')\n",
    "    # call 1 channel\n",
    "        # use the list_comprehension for bias_mV range\n",
    "        # list comprehension is more faster\n",
    "        # after rotation, resize = False! , or replacement size error! \n",
    "        # replace the channel(padded) 3D data as a new 3D (rotated )data set \n",
    "\n",
    "    for ch in xrdata_pad:\n",
    "        xrdata_rot[ch].values = skimage.transform.rotate(xrdata[ch].values.astype('float64'),\n",
    "                                                         rotation_angle,\n",
    "                                                         cval =xrdata[ch].to_numpy().min(),\n",
    "                                                         resize = True)\n",
    "    return xrdata_rot\n",
    "# ### average X or Y direction jof Grid_3D dataset \n",
    "# * use xr_data (3D)\n",
    "# * average_in = 'X' or 'Y'\n",
    "# * ch_l_name = channel name for line profile  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274d0562-72b0-4787-a2ad-ce450ef3b05f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def grid3D_line_avg_pks (xr_data, average_in =  'X',\n",
    "                         ch_l_name = 'LIX_unit_calc',\n",
    "                         height = None,\n",
    "                         distance = None,\n",
    "                         threshold = None,\n",
    "                         prominence=None, width=None,\n",
    "                         padding_value = np.nan,\n",
    "                         window_length=7,\n",
    "                         polyorder=3\n",
    "                        ) : \n",
    "\n",
    "    if average_in ==  'X':\n",
    "        mean_direction = 'X'\n",
    "        line_direction = 'Y'\n",
    "        print('line_direction == Y')\n",
    "    elif average_in ==  'Y': \n",
    "        mean_direction = 'Y'\n",
    "        line_direction = 'X'\n",
    "        print('line_direction == X')\n",
    "    else: print ('check the line STS direction in 3D dataset ')\n",
    "\n",
    "    xr_data_l = xr_data.mean( dim = mean_direction )\n",
    "    xr_data_l.attrs = xr_data.attrs.copy()\n",
    "    # add attrs manually \n",
    "\n",
    "    ### find peaks & pad \n",
    "    #* use the SG filter \n",
    "    #* derivative (dim = 'bias_mV') twice \n",
    "    #* find peaks & padding \n",
    "\n",
    "    xr_data_l_pks=  peak_pad(\n",
    "        find_peaks_xr(\n",
    "            savgolFilter_xr(\n",
    "                savgolFilter_xr(\n",
    "                    xr_data_l.differentiate(coord='bias_mV'),\n",
    "                    window_length=window_length,polyorder=polyorder,\n",
    "                ).differentiate(coord='bias_mV'),\n",
    "                window_length=window_length,polyorder=polyorder\n",
    "            )*-1, height =height, distance = distance, threshold = threshold, prominence=prominence, width=width), \n",
    "        padding_value = padding_value)\n",
    "    if average_in ==  'X':\n",
    "        xr_data_l_pks.attrs['line_direction'] ='Y'\n",
    "    elif average_in ==  'Y': \n",
    "        xr_data_l_pks.attrs['line_direction'] ='X'\n",
    "    else: print ('check the line STS direction in 3D dataset ')\n",
    "    # smooth, deriv, smooth, derive, find peak, padding \n",
    "    #xr_data_l_pks\n",
    "    \n",
    "    \n",
    "    # in the xr_data_l_pks\n",
    "    # choose a particular channel after pean & pad \n",
    "    # replace the channel to original xrdata \n",
    "    # xr_data_l_pks contains 2nd derivative results \n",
    "    \n",
    "    for ch_names in xr_data:\n",
    "        xr_data_l_pks[ch_names] =  xr_data_l [ch_names]\n",
    "    \n",
    "    \n",
    "    return xr_data_l_pks\n",
    "#grid_3D_test_l_pk = grid3D_line_avg_pks(grid_3D, average_in= 'Y')\n",
    "#grid_3D_test_l_pk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dfae91-0512-491c-8a72-5afe85aae299",
   "metadata": {},
   "source": [
    "### Plot line profile (line offset + peak positions) \n",
    "### only after after **grid3D_line_avg_pks**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90ba42b9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79511631-efe4-4a99-98d0-daecd3d5a628",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def  grid_lineNpks_offset(xr_data_l_pks, \n",
    "                          ch_l_name = 'LIX_unit_calc',\n",
    "                          plot_y_offset= 2E-11, \n",
    "                          peak_LIX_min = 1E-13, \n",
    "                          fig_size = (6,8), \n",
    "                          legend_title = None):\n",
    "    # add peak point one-by-one (no palett func in sns)\n",
    "    #  after find peak & padding\n",
    "    # use choose the channel to offset-plot \n",
    "    # use the plot_y_offset to adjust the offset values \n",
    "    ch_l_name = ch_l_name\n",
    "    ch_l_pk_name = ch_l_name +'_peaks_pad'\n",
    "    line_direction = xr_data_l_pks.line_direction\n",
    "    plot_y_offset  =  plot_y_offset\n",
    "    \n",
    "    sns_color_palette = \"rocket\"\n",
    "    #color map for fig\n",
    "    \n",
    "    #xr_data_l_pks\n",
    "    ### prepare XR dataframe for line spectroscopy plot \n",
    "    xr_data_l_pks_ch_slct = xr_data_l_pks[[ch_l_name,ch_l_pk_name]]\n",
    "    # choose the 2 channels from 2nd derivative (to maintain the coords info) \n",
    "\n",
    "\n",
    "    #line_direction check again \n",
    "    \n",
    "    if xr_data_l_pks.line_direction == 'Y': \n",
    "        spacing = xr_data_l_pks_ch_slct.Y_spacing\n",
    "    elif xr_data_l_pks.line_direction == 'X': \n",
    "        spacing = xr_data_l_pks_ch_slct.X_spacing\n",
    "    else : \n",
    "        print('check direction & X or Y spacing for offset') \n",
    "\n",
    "    xr_data_l_pks_ch_slct['offset'] = (xr_data_l_pks_ch_slct[line_direction] - xr_data_l_pks_ch_slct[line_direction].min())/spacing\n",
    "    # prepare offset index channnel \n",
    "    print (' plot_y_offset  to adjust line-by-line spacing')\n",
    "\n",
    "    xr_data_l_pks_ch_slct[ch_l_name+'_offset'] = xr_data_l_pks_ch_slct[ch_l_name] + plot_y_offset * xr_data_l_pks_ch_slct['offset']\n",
    "    # offset the curve b\n",
    "    print (xr_data_l_pks_ch_slct)\n",
    "    \n",
    "\n",
    "    ch_l_name_df_list = [] \n",
    "    ch_l_name_pks_df_list = []\n",
    "    # prepare empty list to append dataframes in the for - loop (y_i or x_i)\n",
    "\n",
    "    #line_direction check again \n",
    "    #########################\n",
    "    # line_diection check\n",
    "    if xr_data_l_pks_ch_slct.line_direction == 'Y': \n",
    "        lines  = xr_data_l_pks_ch_slct.Y\n",
    "\n",
    "        for y_i, y_points in enumerate (lines):\n",
    "\n",
    "            # set min peak height (LIX amplitude =  resolution limit)\n",
    "\n",
    "            y_i_pks  = xr_data_l_pks_ch_slct[ch_l_pk_name].isel(Y = y_i).dropna(dim='peaks').astype('int32')\n",
    "            # at (i_th )Y position, select peak index for bias_mV\n",
    "            real_pks_mask = (xr_data_l_pks_ch_slct.isel(Y = y_i, bias_mV = y_i_pks.values)[ch_l_name] > peak_LIX_min).values\n",
    "            # prepare a 'temp' mask for each Y position \n",
    "            y_i_pks_slct =  y_i_pks.where(real_pks_mask).dropna(dim='peaks').astype('int32')\n",
    "            # y_i_pks_slct with mask selection  \n",
    "\n",
    "            ch_l_name_y_i_df = xr_data_l_pks_ch_slct[ch_l_name+'_offset'].isel(Y = y_i).to_dataframe()\n",
    "            # LIX_offset  at Y_i position \n",
    "            ch_l_name_df_list.append(ch_l_name_y_i_df)\n",
    "            \n",
    "            ch_l_name_y_i_pks_df = xr_data_l_pks_ch_slct.isel(Y = y_i, bias_mV = y_i_pks_slct.values)[ch_l_name+'_offset'].to_dataframe()\n",
    "            # selected peaks with offest Y \n",
    "            ch_l_name_pks_df_list.append(ch_l_name_y_i_pks_df)\n",
    "            \n",
    "            # data at selected Y, & peak position, LIX_offset\n",
    "            \n",
    "    #########################\n",
    "    # line_diection check\n",
    "\n",
    "    elif xr_data_l_pks_ch_slct.line_direction == 'X': \n",
    "        lines = xr_data_l_pks_ch_slct.X\n",
    "\n",
    "        for x_i, x_points in enumerate (lines):\n",
    "\n",
    "            # set min peak height (LIX amplitude =  resolution limit)\n",
    "\n",
    "            x_i_pks  = xr_data_l_pks_ch_slct[ch_l_pk_name].isel(X = x_i).dropna(dim='peaks').astype('int32')\n",
    "            # at (i_th )X position, select peak index for bias_mV\n",
    "            real_pks_mask = (xr_data_l_pks_ch_slct.isel(X = x_i, bias_mV = x_i_pks.values)[ch_l_name] > peak_LIX_min).values\n",
    "            # prepare a 'temp' mask for each X position \n",
    "            x_i_pks_slct =  x_i_pks.where(real_pks_mask).dropna(dim='peaks').astype('int32')\n",
    "            # x_i_pks_slct with mask selection  \n",
    "\n",
    "            ch_l_name_x_i_df = xr_data_l_pks_ch_slct[ch_l_name+'_offset'].isel(X = x_i).to_dataframe()\n",
    "            # LIX_offset  at X_i position \n",
    "            ch_l_name_df_list.append(ch_l_name_x_i_df)\n",
    "            ch_l_name_x_i_pks_df = xr_data_l_pks_ch_slct.isel(X = x_i, bias_mV = x_i_pks_slct.values)[ch_l_name+'_offset'].to_dataframe()\n",
    "            ch_l_name_pks_df_list.append(ch_l_name_x_i_pks_df)\n",
    "            \n",
    "            # selected peaks with offest X \n",
    "            \n",
    "    else : \n",
    "        print('check direction & X or Y spacing for offset') \n",
    "    \n",
    "    ch_l_name_df = pd.concat(ch_l_name_df_list).reset_index()\n",
    "    ch_l_name_pks_df = pd.concat(ch_l_name_pks_df_list).reset_index()\n",
    "    \n",
    "    fig,ax = plt.subplots(figsize = fig_size)\n",
    "\n",
    "    sns.lineplot(data = ch_l_name_df,\n",
    "                         x ='bias_mV', \n",
    "                         y = ch_l_name+'_offset',\n",
    "                         palette = \"rocket\",\n",
    "                         hue = xr_data_l_pks.line_direction,\n",
    "                         ax = ax,legend='full')\n",
    "\n",
    "    sns.scatterplot(data = ch_l_name_pks_df,\n",
    "                            x ='bias_mV',\n",
    "                            y = ch_l_name+'_offset',\n",
    "                            palette =\"rocket\",sizes=0.2,\n",
    "                            hue = xr_data_l_pks.line_direction,\n",
    "                            ax = ax,legend='full')\n",
    "    # legend control!( cut the handles 1/2)\n",
    "    ax.set_xlabel('Bias (mV)')   \n",
    "    #ax.set_ylabel(ch_l_name+'_offset')   \n",
    "    ax.set_ylabel('LDOS')   \n",
    "    handles0, labels0 = ax.get_legend_handles_labels()\n",
    "    handles1 = handles0[:int(len(handles0)//2)]\n",
    "    labels1 = [ str(round(float(label)*1E9,2)) for label in labels0[:int(len(labels0)//2)] ] \n",
    "    handles2 = handles1[::5][::-1]\n",
    "    labels2 = labels1[::5][::-1]\n",
    "    # convert the line length as nm\n",
    "    print(labels2)\n",
    "    ax.legend(handles2,   labels2, title = legend_title)\n",
    "    # use the half of legends (line + scatter) --> use lines only\n",
    "    #plt.show()\n",
    "    return xr_data_l_pks_ch_slct, ch_l_name_df, ch_l_name_pks_df, fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637f0cb5",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcf4d6f-9ba1-45f0-baad-52ff30938a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check drift compensation result for each lines. \n",
    "# use correlation between selected areas. \n",
    "\n",
    "# not fully "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc0e55ab-0d8d-4898-8efe-4e1eb4329f55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def padding_xr (xrdata, padding_dim = 'X', padding_shape =5, padding_value = np.nan): \n",
    "    '''\n",
    "    Input: \n",
    "    xrdata : use padding result only for XY\n",
    "    convert XY coord accordingly \n",
    "    padding_value : filling value, can be other constant, update if needed.\n",
    "    \n",
    "    Output:  padding result \n",
    "    '''\n",
    "    \n",
    "    # padding \n",
    "\n",
    "    # padding amount:  padding_shape \n",
    "    padding_shape = padding_shape \n",
    "    \n",
    "    # front side append for X dimension \n",
    "    X_pad  = np.append (\n",
    "        np.append(\n",
    "            (np.arange(1,padding_shape+1,1)[::-1]* -xrdata.X_spacing)\n",
    "            - xrdata.X.min().values , \n",
    "            xrdata.X ), \n",
    "        (np.arange(1,padding_shape+1,1)* xrdata.X_spacing)\n",
    "        + xrdata.X.max().values)\n",
    "    Y_pad  = np.append (\n",
    "        np.append(\n",
    "            (np.arange(1,padding_shape+1,1)[::-1]* -xrdata.Y_spacing)\n",
    "            - xrdata.Y.min().values , \n",
    "            xrdata.Y ), \n",
    "        (np.arange(1,padding_shape+1,1)* xrdata.Y_spacing)\n",
    "        + xrdata.Y.max().values)\n",
    "        # np.arange start from 1 & end with +1 to match existing X arange min & max)\n",
    "    # or we will see iterated min & max values \n",
    "    # use append function twice for front & back side \n",
    "\n",
    "    \n",
    "    if padding_dim == 'X': \n",
    "    \n",
    "        # front side append for X dimension \n",
    "        xrdata_pad = xrdata.pad(X=(padding_shape,padding_shape), constant_values = np.nan)\n",
    "        xrdata_pad = xrdata_pad.assign_coords(X = X_pad)\n",
    "    elif padding_dim == 'Y':\n",
    "        # Y_pad only     \n",
    "        xrdata_pad = xrdata.pad(Y=(padding_shape,padding_shape),constant_values = np.nan)\n",
    "        xrdata_pad = xrdata_pad.assign_coords(Y = Y_pad)\n",
    "    \n",
    "    elif padding_dim == 'XY':\n",
    "        xrdata_pad = xrdata.pad(X=(padding_shape,padding_shape), Y=(padding_shape,padding_shape),constant_values = np.nan)\n",
    "        xrdata_pad = xrdata_pad.assign_coords(X = X_pad)\n",
    "        xrdata_pad = xrdata_pad.assign_coords(Y = Y_pad)\n",
    "    else: \n",
    "        print(\"choose padding_dim = 'X', 'Y', or 'XY'\")\n",
    "        \n",
    "    return xrdata_pad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8987f493-acc9-4fad-8bc4-192d69839631",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def drift_compensation_y_topo_crrltn (xr_data, y_sub_n=10, padding_shape = 10, drift_interpl_method='nearest'): \n",
    "    '''\n",
    "    input: xr 3D data \n",
    "    use grid_xr.topography to calculate Y drift with drift \n",
    "    \n",
    "    use padding_xr function with padding_dims= 'X' only\n",
    "    \n",
    "    output: Xr 3D data + padding \n",
    "    \n",
    "    \n",
    "    '''\n",
    "    xr_data_topo = xr_data.topography\n",
    "    xr_data_topo.attrs = xr_data.attrs\n",
    "    xr_data_pad = padding_xr(xr_data, padding_dim = 'X', padding_shape = padding_shape)\n",
    "\n",
    "    y_N = len (xr_data_topo.Y)\n",
    "\n",
    "    y_sub_n = y_sub_n\n",
    "    drift_interpl_method='nearest'\n",
    "\n",
    "    # use xr_data for correlation search \n",
    "    # apply the correlation shift to the pad results \n",
    "\n",
    "\n",
    "    #y_j = 0 \n",
    "    offset = np.array([0, y_N//2])\n",
    "    # use for loop \n",
    "    print ('drift check with for topography channel(2D data), apply to 3D data')\n",
    "    for y_j  in range (len (xr_data_topo.Y)//y_sub_n - 1) :\n",
    "        y_N = len (xr_data_topo.Y)\n",
    "        #print (y_j)\n",
    "\n",
    "        Y_sub_n0 = y_j*y_sub_n * xr_data_topo.Y_spacing\n",
    "        Y_sub_n1 = (y_j+1)*y_sub_n * xr_data_topo.Y_spacing\n",
    "        Y_sub_n2 = (y_j+2)*y_sub_n * xr_data_topo.Y_spacing\n",
    "        #print (Y_sub_n0, Y_sub_n1, Y_sub_n2)\n",
    "        # check Y drift comparision area \n",
    "        # use y_sub_n = 5 ==> 0-5, 6-10, 10-5, ... \n",
    "        line0 = xr_data_topo.where(xr_data_topo.Y >= Y_sub_n0, drop = True).where (xr_data_topo.Y < Y_sub_n1, drop = True )\n",
    "        line1 = xr_data_topo.where(xr_data_topo.Y >=  Y_sub_n1, drop = True).where (xr_data_topo.Y <  Y_sub_n2, drop = True )\n",
    "        # select two region for correlation search \n",
    "        corrl_line0_line1 = sp.signal.correlate2d(line0.values, line1.values, mode = 'same')#  use mode = same area to use the line0 X& Y value\n",
    "        # search for the correlation. if correlation is not center --> drift. \n",
    "        # but.. there will be an step edge (horizontal), or atomic lattice --> y_sub_n << atomic lattice \n",
    "        ind_max = np.array (np.unravel_index(np.argmax(corrl_line0_line1, axis=None), corrl_line0_line1.shape)) # find max point \n",
    "        # find argmax index point\n",
    "        #print (ind_max)\n",
    "        offset = np.vstack ([offset, ind_max])\n",
    "\n",
    "    offset_0 = offset[: , -1] -  y_N//2\n",
    "    # check offset from center \n",
    "    #offset_accumulation  = [ offset_0[:n+1].sum()  for n in range (len(offset_0)) ]\n",
    "\n",
    "    offset_accumulation  = np.array ( [ offset_0[:n].sum()  \n",
    "                                   for n in range (len(offset_0)+1) ])*xr_data_topo.Y_spacing \n",
    "    # offset is from between two region.. get an accumlated offset. for whole Y axis. \n",
    "    offset_accumulation_df =pd.DataFrame (\n",
    "    np.vstack ([ np.array ([ y_j *y_sub_n *xr_data_topo.Y_spacing  \n",
    "                            for y_j in range(len (xr_data_topo.Y)//y_sub_n+1) ]), \n",
    "                offset_accumulation]).T, columns  =['Y','offset_X'])\n",
    "    offset_accumulation_xr  = offset_accumulation_df.set_index('Y').to_xarray()\n",
    "\n",
    "    offset_accumulation_xr_intrpl = offset_accumulation_xr.offset_X.interp(Y = xr_data_topo.Y.values,  method=drift_interpl_method)\n",
    "    # accumluted offset--> covert to df , xr, \n",
    "    # accumnulated offset to compensate in X \n",
    "    # use interpolation along Y --> point offset calc ==> apply to all y points. \n",
    "\n",
    "    #offset_accumulation_xr_intrpl.plot()\n",
    "\n",
    "    # for each lines, adjust value after offset compensated  ==> interpolated again. \n",
    "    xr_data_topo_offset = xr_data_topo.copy(deep= True)\n",
    "    # dont forget deep copy... \n",
    "\n",
    "    #offset_accumulation_xr_intrpl\n",
    "    # for each lines, adjust value after offset compensated  ==> interpolated again. \n",
    "    xr_data_offset = xr_data_pad.copy(deep= True)\n",
    "    # dont forget deep copy... \n",
    "    # used padding xr result for applying offset \n",
    "\n",
    "\n",
    "    for ch_i, ch_name in enumerate (xr_data_pad): \n",
    "        print(ch_i, ch_name)\n",
    "        # adjust y drift of all channels\n",
    "        #xr_data_pad[ch_name]\n",
    "        for y_j, y  in enumerate (xr_data_pad[ch_name].Y):\n",
    "            new_x_i =  xr_data_offset.isel (Y=y_j).X - offset_accumulation_xr_intrpl.isel(Y=y_j)\n",
    "            # for each y axis. shift X position \n",
    "            #new_x_i\n",
    "            xr_data_offset_ch_y_j = xr_data_pad[ch_name].isel (Y=y_j).assign_coords({\"X\": new_x_i}) \n",
    "            #xr_data_offset_y_j\n",
    "\n",
    "            # assign_coord as a new calibrated offset-X coords\n",
    "            xr_data_offset_ch_y_j_intp = xr_data_offset_ch_y_j.interp(X = xr_data_offset.X)\n",
    "            #xr_data_offset_y_j_intp\n",
    "\n",
    "            xr_data_pad[ch_name][dict(Y = y_j)]= xr_data_offset_ch_y_j_intp\n",
    "            #grid_topo_offset.isel(Y=y_j).topography.values = grid_topo_offest_y_j_intp.topography\n",
    "            # use [dict()] for assign values , instead of isel() \n",
    "            # isel is not working... follow the instruction manual in web.!\n",
    "    #xr_data_pad\n",
    "\n",
    "    fig,axs = plt.subplots(ncols = 2, figsize = (8,3))\n",
    "    xr_data.topography.plot(ax =axs[0], robust = True)\n",
    "    xr_data_pad.topography.plot(ax =axs[1], robust = True)\n",
    "    plt.show()\n",
    "    return xr_data_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115408b7-0eb1-4c70-9f4c-65ff0305fc8a",
   "metadata": {
    "lines_to_next_cell": 1,
    "tags": []
   },
   "outputs": [],
   "source": [
    "### Xr rotation function \n",
    "# rotate the XY plan in xr data \n",
    "def rotate_3D_fft_xr (xrdata, rotation_angle): \n",
    "    # padding first \n",
    "    for ch_i,ch_name in enumerate (xrdata):\n",
    "        if ch_i == 0:  # use only the first channel to calculate a padding size \n",
    "            padding_shape = skimage.transform.rotate(xrdata[ch_name].values.astype('float64'),\n",
    "                                                     rotation_angle,\n",
    "                                                     resize = True).shape[:2]\n",
    "            # After rotation, still 3D shape ->  [:2]\n",
    "            \n",
    "            padding_xy = (np.array( padding_shape)-np.array(xrdata[ch_name].shape[:2]) +1)/2\n",
    "            padding_xy = padding_xy.astype(int)\n",
    "    xrdata_pad = xrdata.pad(freq_X=(padding_xy[0],padding_xy[0]), \n",
    "                            freq_Y =(padding_xy[1],padding_xy[1]),\n",
    "                            mode='constant',\n",
    "                            cval = xrdata.min())\n",
    "    if np.array(xrdata_pad[ch_name]).shape[:2] != padding_shape:\n",
    "        # in case of xrdata_pad shape is +1 larger than real padding_shape\n",
    "        # index 다루는 법  (X)\n",
    "        x_spacing = np.diff(xrdata.freq_X).mean()\n",
    "        y_spacing = np.diff(xrdata.freq_Y).mean()\n",
    "        xrdata.freq_X[0]\n",
    "        xrdata.freq_Y[0]\n",
    "\n",
    "        x_pad_dim = padding_shape[0]#int(padding_xy[0]*2+xrdata.X.shape[0])\n",
    "        y_pad_dim = padding_shape[1]#int(padding_xy[0]*2+xrdata.Y.shape[0])\n",
    "\n",
    "        x_pad_arr =  np.linspace(-1*padding_xy[0]*x_spacing, x_spacing*x_pad_dim,x_pad_dim+1)\n",
    "        y_pad_arr =  np.linspace(-1*padding_xy[1]*y_spacing, y_spacing*y_pad_dim,y_pad_dim+1)\n",
    "\n",
    "        # 0 에서 전체 크기 만큼 padding 한결과를 array 만들고 offset 은 pad_x 만큼 \n",
    "        x_pad_arr.shape\n",
    "        y_pad_arr.shape\n",
    "        xrdata_pad = xrdata_pad.assign_coords( {\"freq_X\" :  x_pad_arr}).assign_coords({\"freq_Y\" :  y_pad_arr})\n",
    "        xrdata_rot = xrdata_pad.sel(freq_X = xrdata_pad.freq_X[:-1].values, freq_Y = xrdata_pad.freq_Y[:-1].values)\n",
    "        print ('padding size != rot_size')\n",
    "    else : # np.array(xrdata_pad[ch_name]).shape == padding_shape \n",
    "            # in case of xrdata_pad shape is +1 larger than real padding_shape\n",
    "\n",
    "        # index 다루는 법  (X)\n",
    "        x_spacing = np.diff(xrdata.freq_X).mean()\n",
    "        y_spacing = np.diff(xrdata.freq_Y).mean()\n",
    "        xrdata.freq_X[0]\n",
    "        xrdata.freq_Y[0]\n",
    "\n",
    "        x_pad_dim = padding_shape[0]#int(padding_xy[0]*2+xrdata.X.shape[0])\n",
    "        y_pad_dim = padding_shape[1]#int(padding_xy[0]*2+xrdata.Y.shape[0])\n",
    "\n",
    "        x_pad_arr =  np.linspace(-1*padding_xy[0]*x_spacing, x_spacing*x_pad_dim,x_pad_dim)\n",
    "        y_pad_arr =  np.linspace(-1*padding_xy[1]*y_spacing, y_spacing*y_pad_dim,y_pad_dim)\n",
    "\n",
    "        # 0 에서 전체 크기 만큼 padding 한결과를 array 만들고 offset 은 pad_x 만큼 \n",
    "        x_pad_arr.shape\n",
    "        y_pad_arr.shape\n",
    "        xrdata_pad = xrdata_pad.assign_coords( {\"freq_X\" :  x_pad_arr}).assign_coords({\"freq_Y\" :  y_pad_arr})\n",
    "        xrdata_rot = xrdata_pad.copy()      \n",
    "        print ('padding size == rot_size')\n",
    "    # call 1 channel\n",
    "        # use the list_comprehension for bias_mV range\n",
    "        # list comprehension is more faster\n",
    "        # after rotation, resize = False! , or replacement size error! \n",
    "        # replace the channel(padded) 3D data as a new 3D (rotated )data set \n",
    "\n",
    "    for ch in xrdata_pad:\n",
    "        xrdata_rot[ch].values = skimage.transform.rotate(xrdata[ch].values.astype('float64'),\n",
    "                                                         rotation_angle,\n",
    "                                                         cval =xrdata[ch].to_numpy().min(),\n",
    "                                                         resize = True)\n",
    "    return xrdata_rot\n",
    "# ### average X or Y direction jof Grid_3D dataset \n",
    "# * use xr_data (3D)\n",
    "# * average_in = 'X' or 'Y'\n",
    "# * ch_l_name = channel name for line profile  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e448539-2be7-4ac9-8a0b-77296551e4e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hv_fft_bias_mV_slicing(xr_data,ch = 'LDOS_fb_fft',frame_width = 200,cmap = 'bwr'): \n",
    "    '''\n",
    "    input : xarray dataset \n",
    "    output : holoview image\n",
    "    \n",
    "    * slicing 3D data set in XY plane \n",
    "    * bias_mV is knob\n",
    "    \n",
    "    default channel  =  'LIX_fb',  or assgin 'I_fb' or 'LDOS_fb'\n",
    "    default setting for frame width and cmap  can be changed. \n",
    "    \n",
    "    if you need to add color limit \n",
    "        add \".opts(clim=(0, 1E-10))\"\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    import holoviews as hv\n",
    "    from holoviews import opts\n",
    "\n",
    "    xr_data_hv = hv.Dataset(xr_data[ch])\n",
    "\n",
    "    hv.extension('bokeh')\n",
    "    ###############\n",
    "    # bias_mV slicing\n",
    "    dmap_plane  = [\"freq_X\",\"freq_Y\"]\n",
    "    dmap = xr_data_hv.to(hv.Image,\n",
    "                         kdims = dmap_plane,\n",
    "                         dynamic = True )\n",
    "    dmap.opts(colorbar = True,\n",
    "              cmap = 'bwr',\n",
    "              frame_width = frame_width,\n",
    "              aspect = 'equal').relabel('XY plane slicing: ')\n",
    "    fig = hv.render(dmap)\n",
    "    return dmap   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37dc8e1c-f9c3-4006-946e-cec7931b3291",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hv_fft_XY_slicing(xr_data,ch = 'LDOS_fb_fft', slicing= 'X', frame_width = 200,cmap = 'bwr'): \n",
    "    '''\n",
    "    input : xarray dataset \n",
    "    output : holoview image \n",
    "    \n",
    "    \n",
    "    * slicing 3D data set in X-bias_mV or Y-bias_mV plane \n",
    "    * X or Y position is knob\n",
    "    \n",
    "    \n",
    "    default channel  =  'LIX_fb',  or assgin 'I_fb'\n",
    "    default setting for frame width and cmap  can be changed. \n",
    "    if you need to add color limit \n",
    "     \n",
    "    add \".opts(clim=(0, 1E-10))\"\n",
    "    \n",
    "    '''\n",
    "    import holoviews as hv\n",
    "    from holoviews import opts\n",
    "\n",
    "    xr_data_hv = hv.Dataset(xr_data[ch])\n",
    "\n",
    "    hv.extension('bokeh')\n",
    "    ###############\n",
    "    # bias_mV slicing\n",
    "    if slicing == 'freq_Y':\n",
    "        dmap_plane  = [ \"freq_X\",\"freq_bias_mV\"]\n",
    "\n",
    "        dmap = xr_data_hv.to(hv.Image,\n",
    "                             kdims = dmap_plane,\n",
    "                             dynamic = True )\n",
    "        dmap.opts(colorbar = True,\n",
    "                  cmap = 'bwr',\n",
    "                  frame_width = frame_width).relabel('X - bias_mV plane slicing: ')\n",
    "    else : #slicing= 'freq_X'\n",
    "        dmap_plane  = [ \"freq_Y\",\"freq_bias_mV\"]\n",
    "\n",
    "        dmap = xr_data_hv.to(hv.Image,\n",
    "                             kdims = dmap_plane,\n",
    "                             dynamic = True )\n",
    "        dmap.opts(colorbar = True,\n",
    "                  cmap = 'bwr',\n",
    "                  frame_width = frame_width).relabel('Y - bias_mV plane slicing: ')\n",
    "    fig = hv.render(dmap)\n",
    "    return dmap   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "65173c76-e311-4d20-8cbb-bfab3a28a586",
   "metadata": {
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def hv_fft_bbox_crop (xr_data, bound_box , ch = 'LDOS_fb_fft' ,slicing_bias_mV = 0.5):\n",
    "    '''\n",
    "    use cropped BBox area \n",
    "    freq_X or freq_Y  vs  Bias plot \n",
    "    \n",
    "    '''\n",
    "    import holoviews as hv\n",
    "    from holoviews import opts\n",
    "    hv.extension('bokeh')\n",
    "    # slicing bias_mV = 5 mV\n",
    "    \n",
    "    #bound_box.bounds\n",
    "    x_bounds_msk = (xr_data.freq_X > bound_box.bounds[0] ) & (xr_data.freq_X < bound_box.bounds[2])\n",
    "    y_bounds_msk = (xr_data.freq_Y > bound_box.bounds[1] ) & (xr_data.freq_Y < bound_box.bounds[3])\n",
    "\n",
    "    xr_data_bbox = xr_data.where (xr_data.freq_X[x_bounds_msk] + xr_data.freq_Y[y_bounds_msk])\n",
    "    \n",
    "    fig,axs = plt.subplots (nrows = 1,\n",
    "                            ncols = 2,\n",
    "                            figsize = (12,4))\n",
    "\n",
    "    isns.imshow(xr_data[ch].sel(freq_bias_mV = slicing_bias_mV, method=\"nearest\" ),\n",
    "                ax =  axs[0],\n",
    "                robust = True)\n",
    "\n",
    "    # add rectangle for bbox \n",
    "    from matplotlib.patches import Rectangle\n",
    "    # find index value of bound box \n",
    "\n",
    "    Bbox_x0 = np.abs((xr_data.freq_X-bound_box.bounds[0]).to_numpy()).argmin()\n",
    "    Bbox_y0 = np.abs((xr_data.freq_Y-bound_box.bounds[1]).to_numpy()).argmin()\n",
    "    Bbox_x1 = np.abs((xr_data.freq_X-bound_box.bounds[2]).to_numpy()).argmin()\n",
    "    Bbox_y1 = np.abs((xr_data.freq_Y-bound_box.bounds[3]).to_numpy()).argmin()\n",
    "    Bbox = Bbox_x0,Bbox_y0,Bbox_x1,Bbox_y1\n",
    "    # substract value, absolute value with numpy, argmin returns index value\n",
    "\n",
    "    # when add rectangle, add_patch used index \n",
    "    axs[0].add_patch(Rectangle((Bbox_x0 , Bbox_y0 ), \n",
    "                               Bbox_x1 -Bbox_x0 , Bbox_y1-Bbox_y0,\n",
    "                               edgecolor = 'red',\n",
    "                               fill=False,\n",
    "                               lw=2,\n",
    "                               alpha=1))\n",
    "\n",
    "    isns.imshow(xr_data_bbox[ch].sel(freq_bias_mV = slicing_bias_mV, method=\"nearest\" ),\n",
    "                ax =  axs[1],\n",
    "                robust = True)\n",
    "    #sns.lineplot(x = \"freq_bias_mV\",\n",
    "    #             y = ch, \n",
    "    #             data = xr_data_bbox.to_dataframe(),\n",
    "    #             ax = axs[2])\n",
    "    #plt.savefig('grid011_bbox)p.png')\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "    return xr_data_bbox"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d08e017f-8f46-4834-86e6-3ebfde19eb67",
   "metadata": {},
   "source": [
    "### Not Used Previous (WORKING)Functions (FOR 3d ONLY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adee9607-306e-46c4-840d-94d47308aa79",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def savgolFilter_xr(xrdata,window_length=7,polyorder=3): \n",
    "    # window_length = odd number\n",
    "    #import copy\n",
    "    #xrdata_prcssd = copy.deepcopy(xrdata)\n",
    "    xrdata_prcssd = xrdata.copy(deep = True)\n",
    "    print('Apply a Savitzky-Golay filter to an xarray Dataset.')\n",
    "    xAxis = xrdata.X.size # or xrdata.dims.mapping['X']\n",
    "    yAxis = xrdata.Y.size\n",
    "    for data_ch in xrdata:\n",
    "        if len(xrdata[data_ch].dims) == 2:\n",
    "            # smoothing filter only for the 3D data set\n",
    "            pass\n",
    "            \n",
    "        else :\n",
    "            print (data_ch)\n",
    "            xrdata_prcssd[data_ch] = xr.DataArray (\n",
    "                np.array ([\n",
    "                    sp.signal.savgol_filter(xrdata[data_ch].isel(X = x, Y = y).values,\n",
    "                                            window_length, \n",
    "                                            polyorder , \n",
    "                                            mode = 'nearest')\n",
    "                    for x in range(xAxis) \n",
    "                    for y in range(yAxis)\n",
    "                ] ).reshape(xAxis,yAxis, xrdata.bias_mV.size),\n",
    "                dims = [\"X\", \"Y\", \"bias_mV\"],\n",
    "                coords = {\"X\": xrdata.X,\n",
    "                          \"Y\": xrdata.Y,\n",
    "                          \"bias_mV\": xrdata.bias_mV}\n",
    "            )\n",
    "    return xrdata_prcssd\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c1c7e9d-ee10-4070-a670-44afaf44671a",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def find_peaks_xr(xrdata): \n",
    "    from scipy.signal import find_peaks\n",
    "    xrdata_prcssd = xrdata.copy(deep = True)\n",
    "    print('Find peaks in STS to an xarray Dataset.')\n",
    "    xAxis = xrdata.X.size\n",
    "    yAxis = xrdata.Y.size\n",
    "    for data_ch in xrdata:\n",
    "        if len(xrdata[data_ch].dims)==2:\n",
    "            # smoothing filter only for the 3D data set\n",
    "            pass\n",
    "            \n",
    "        else :\n",
    "            print (data_ch)\n",
    "            \"\"\"xrdata_prcssd[data_ch+'_peaks']= xr.DataArray(np.ones((xAxis,yAxis), dtype = object),\n",
    "                                                             dims=[\"X\", \"Y\"],\n",
    "                                                             coords={\"X\": xrdata.X, \"Y\": xrdata.Y} )\"\"\"\n",
    "            xrdata_prcssd[data_ch+'_peaks'] = xr.DataArray (\n",
    "                np.array([ find_peaks(xrdata[data_ch].isel(X = x, Y = y).values)[0] \n",
    "                          for x in range(xAxis)  \n",
    "                          for y in range(yAxis)], dtype = object ).reshape(xAxis,yAxis),\n",
    "                dims=[\"X\", \"Y\"],\n",
    "                coords={\"X\": xrdata.X, \"Y\": xrdata.Y})\n",
    "            \n",
    "                    # use the \"xrdata_prcssd[data_ch].values[x,y,:]\"  for \"xrdata_prcssd\" \n",
    "                    # not \".isel(X = x, Y = y).values\"\"\"\n",
    "\n",
    "    return xrdata_prcssd\n",
    "#grid_3D_sg_pks = find_peaks_xr(grid_3D_sg)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:light"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
